---
layout: post
title: Green ML
category : 
tags : 
tagline: 
---

How can we effectively deploy and monitor machine learning models? Particularly if we care more about automation and self-improving over performance?

One way is to have a stream-only mechanism, and just ignore all batch effects. This is the mindset of the [river library](https://github.com/online-ml/river). 

In this instance, the data which arrives for training is assumed to be on a stream, and the data which arrives for scoring is also a stream. In which case, the monitoring is over the various streams only. 

One of the most challenging part of this approach is the upstream data must be stream friendly and pre-flattened. This is not always a possibility, but it would simplify the assumptions and "MLOps" needed for production. 

In order to resolve this issue, the most naive solution is to create features "on-demand". 

```
# create modeling datasets
make backfill FROM="<start date>" TO="<end date>"

# we somehow have to create labels as well
make label FROM="<start date>" TO="<end date>"

# train from modeling dataset and labels + test
# generally this has to be handcraft
python train.py
```

When we are scoring, the simpliest approach is to run:

```
make features
make score
```

Kedro and other frameworks provide lots of prescriptive ways for managing these pipelines, though having a simple CLI could enable this fairly easily. 

However, equalising an online feature store and offline (as above) is notoriously difficult, and assumptions will have to be made. My "go to" assumption is around "best efforts". Perhaps feature pipeline is ran continuously to keep the online feature store up to date. 

The challenge for scoring on a stream (and by extension monitoring), is that we have to have a consistent way to "snapshot" what things looked like when the score is generated, which could be inconsistent compared with running it in batch (not due to programming errors, but could be due to system errors). 

There is still much to think about here. 