---
layout: post
title: thoughtlog
---

[90 day plan for management](http://firstround.com/review/this-90-day-plan-turns-engineers-into-remarkable-managers/)

[Terry Tao](http://www.nytimes.com/2015/07/26/magazine/the-singular-mind-of-terry-tao.html?_r=0)

[github data science](http://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/)

[break out careers](http://www.breakoutcareers.com/)

[case class vs class](http://stackoverflow.com/questions/2312881/what-is-the-difference-between-scalas-case-class-and-class)

Practical applications of DBSCAN at [Netflix](http://techblog.netflix.com/2015/07/tracking-down-villains-outlier.html).

---

> "Principal Component Analysis" is a dimensionally invalid method that gives people a delusion that they are doing something useful with their data. If you change the units that one of the variables is measured in, it will change all the "principal components"! It's for that reason that I made no mention of PCA in my book. I am not a slavish conformist, regurgitating whatever other people think should be taught. I think before I teach. David J C MacKay.

---

[Will Our Understanding of Math Deteriorate Over Time?](http://blog.computationalcomplexity.org/2015/07/will-our-understanding-of-math.html)

comments from [hacker.news]():

> This is a very good and thought-provoking essay for a short blog post, and I have already shared it in a Facebook community heavily populated by professional mathematicians (where the moderator, with a Ph. D. in math from Berkeley, has given it a thumbs up). Thanks for sharing.
>
> I really like the overall point of the post that mathematics once known can be forgotten or neglected, and mathematics written up for mathematics journals can be difficult to understand. Professor John Stillwell writes, in the preface to his book Numbers and Geometry (New York: Springer-Verlag, 1998):
>
> "What should every aspiring mathematician know? The answer for most of the 20th century has been: calculus. . . . Mathematics today is . . . much more than calculus; and the calculus now taught is, sadly, much less than it used to be. Little by little, calculus has been deprived of the algebra, geometry, and logic it needs to sustain it, until many institutions have had to put it on high-tech life-support systems. A subject struggling to survive is hardly a good introduction to the vigor of real mathematics.
>
> ". . . . In the current situation, we need to revive not only calculus, but also algebra, geometry, and the whole idea that mathematics is a rigorous, cumulative discipline in which each mathematician stands on the shoulders of giants.
>
> "The best way to teach real mathematics, I believe, is to start deeper down, with the elementary ideas of number and space. Everyone concedes that these are fundamental, but they have been scandalously neglected, perhaps in the naive belief that anyone learning calculus has outgrown them. In fact, arithmetic, algebra, and geometry can never be outgrown, and the most rewarding path to higher mathematics sustains their development alongside the 'advanced' branches such as calculus. Also, by maintaining ties between these disciplines, it is possible to present a more unified view of mathematics, yet at the same time to include more spice and variety."
>
> Stillwell demonstrates what he means about the interconnectedness and depth of "elementary" topics in the rest of his book, which is a delight to read and full of thought-provoking problems.

---

[How I became an Artist](https://medium.com/@noahbradley/how-i-became-an-artist-4390c6b6656c)

comments from [hacker.news](https://news.ycombinator.com/item?id=9838458):

> The best thing about this piece is the rationality of his decision-making: didn't get a scholarship so take a year off to get better. School was getting too expensive so find a cheaper way to go. Need to pay off student loans so move back in with parents. These are excellent artistic decisions, because they allowed him to practice his craft and never take his eyes off his goal, yet didn't wed him to any particular path forward.
>
> He treated high costs as damage and routed his career around them.
>
> This lesson is as true in business as it is in art: always look for cheaper, more efficient, yet still effective alternatives. Productivity is output/input, and when the output (commercial success, in his case) is a very noisy function of many factors you don't control, figuring out how to minimize input costs while keeping P(success) as high as possible is the key to making good choices. He showed creativity and pragmatism as well as dedication.
>
> Also: failure is always an option (I say this as someone who has failed more often than he has succeeded in his major life goals, although the successes have been more than worth the effort.) Hard work and talent are necessary but not sufficient conditions for success. Maybe there is no audience for the kind of art you love (I'm a formal poet, and have more-or-less made my peace with this.) So make sure you're doing it for love, not money. If the money is there, you've won the lottery, but regardless of that aspect of it, making the art you love, and putting in the hours and deliberate practice to get better at your craft, is worth it.

---

_in response to a post about feeling left behind at 22_

Here's what worked for me. You could say I was an evangelist of sorts. I did not set out to be an evangelist, more to educate myself about devops, and https://sysadmincasts.com was the result. Ended up getting hired by one of the companies I created a video on (but not in an evangelist role).

Here's some things to think about:

- you do not need to be an expert

- you can teach yourself to be an evangelist (probably the best way)

- 22 is _young_ (I have 10 years on you)

- almost better if you know nothing as you learn from the ground up

- you will learn everything you need to know along the way

- possible to brute force your way into it via consistent hard work

Here's my suggested strategy:

Start a blog or youtube channel where you review products around an eco-system. Boom, you now run your own evangelist company, congratulations! Keep at this consistently for a few years. Focus your reviews on a specific type of company or series of product lines. You will get on people's radar. This will bring in job offers as you have proved yourself as an evangelist. A few things that I can think of off the top of my head re: the journey.

- pick an emerging tech eco-system (I chose devops)

- something shiny and new (devops)

- educate yourself about said eco-system

- educate others about what you learned (brain dumps)

- chunk these into small 1-2k word postings (blog or video)

- was doing a weekly text/video release (hope to get back on a schedule)

- it will be extremely painful in the beginning as you will suck (I did)

- you will quickly become better and improve your craft

- have a release cycle to keep you motivated with an end in sight

- set clear goals for what each posting will be about

- show examples (code, workflow, diagrams, etc)

- weekly cycle will force you to keep things small and workable

- just takes time and effort to research

- it's easy to write simple brain dumps about what you learned

- this could take years (it did in my case)

- don't write crap postings or spam content

- we are sick of corporate marketing/hype/vapor crap

- an unpolished human-to-human conversation works very well!

- show example use-cases (who, how, what, why, when, etc)

- think unboxing video for tech products (what does the gui look like)

- people will recognize you provide value

- this will open doors beyond your wildest expectations

- you will gradually become an expert

- very rewarding to help people from around the world

- can take over your life if you let it

- force yourself to keep quality up over all else (don't be lazy)

- you can do this alongside your day job

- builds proof that you can be an evangelist

- you will become much better at distilling concepts

- this provides extreme value for time crunched people

- you taught yourself how to learn & keep current!

The trick is to keep yourself motivated. If you look at many information products today (podcasts, youtube channels, magazines), most of them follow a model like this, just copy it. Just the reward of helping others alone makes this worth it, so you really cannot go wrong! In that, even if you do not become a full-time evangelist, you have educated yourself, educated others, built a public profile, and that will pay off big time when looking for work in the future.

---

[Electron as a way of creating GUI in python.](https://www.fyears.org/2015/06/electron-as-gui-of-python-apps.html)

---

Using [drat](http://dirk.eddelbuettel.com/code/drat.html)

---

Useful - contributes to predictive accuracy
Relevance - measure of bayesian optimal classifier/information gain etc.

---

[More to mathematics rigour](https://terrytao.wordpress.com/career-advice/there%E2%80%99s-more-to-mathematics-than-rigour-and-proofs/)

---

[Lifelong learning according to Richard Hamming](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004020)

---

Networks revision

https://www.udacity.com/course/viewer#!/c-ud819/l-663868561/m-634308999

Some suggestions from Piazza:

- [Network Programming Study Guide](http://www.sal.ksu.edu/faculty/tim/NPstudy_guide/index.html)
- [TCP/IP Illustrated, Volume 1: The Protocols, Addison-Wesley, 1994, ISBN 0-201-63346-9](http://www.kohala.com/start/tcpipiv1.html)
- [Stanford Course: An Introduction to Computer Networks (Fall 2012)](http://f12.class2go.stanford.edu/networking/Fall2012)
- [Computer Networks: 5th Ed. by Tanenbaum & Wetherall](http://cse.hcmut.edu.vn/~minhnguyen/NET/Computer%20Networks%20-%20A%20Tanenbaum%20-%205th%20edition.pdf)

A quick google search turns up these slides for TCP/IP basics:

- http://www.slideshare.net/sanjoysanyal/tcpip-basics

And a few python tutorials on socket programming:

- http://docs.python.org/2/howto/sockets.html
- http://www.tutorialspoint.com/python/python_networking.htm
- TCP: http://www.binarytides.com/python-socket-programming-tutorial
- UDP: http://www.binarytides.com/programming-udp-sockets-in-python

---

```r
a<- bm[bm$treat==1,c("pscores")]
a1<- bm[bm$treat==0,c("pscores")]

a.mean = mean(a)
a.sd = sd(a)

a1.mean = mean(a1)
a1.sd = sd(a1)

X1 <- as.data.frame(list(x = rnorm(100, mean=a.mean, sd=a.sd), treat = 0))
X2 <- as.data.frame(list(x = rnorm(95, mean=a1.mean, sd=a1.sd), treat = 1))

xdf <- rbind(X1, X2)
xdf$treat <- as.factor(xdf$treat)
xdf.treat <- sapply(xdf$treat, function(x) if(x==1) TRUE else FALSE)
matches <- matching(z = xdf.treat, score=xdf$x)

bm <- xdf
am <- xdf[matches$matched,] # after matching dataset

dim(bm)
dim(am)


ggplot(bm, aes(x, ..density.., fill=treat)) + geom_histogram(alpha=0.2) +geom_density(alpha=0.1)
ggplot(am, aes(x, ..density.., fill=treat)) + geom_histogram(alpha=0.2) +geom_density(alpha=0.1)


# using the other matching
library(Matching)
rr <- Match(Tr=xdf.treat, X=xdf$x)
rr1 <- Match(Tr=xdf.treat, X=xdf$x, replace=FALSE)
rr.matched <- c(rr$index.treated, rr$index.control) # matches replace=TRUE
rr1.matched <- c(rr1$index.treated, rr1$index.control) # matches replace=FALSE

am1 <- bm[rr.matched,]
am2 <- bm[rr1.matched,]

ggplot(bm, aes(x, ..density.., fill=treat)) + geom_histogram(alpha=0.2) +geom_density(alpha=0.1)
ggplot(am1, aes(x, ..density.., fill=treat)) + geom_histogram(alpha=0.2) +geom_density(alpha=0.1)
ggplot(am2, aes(x, ..density.., fill=treat)) + geom_histogram(alpha=0.2) +geom_density(alpha=0.1)

dim(am1)
dim(am2)


```

---

# Introduction to Causal Inference

Causal inference is concerned with _what would happen_ to an outcome if we _hypothetically_ did something else to it (i.e. treatment). This idea is commonly used in medicine to determine the effect that medicine might have to a person.

It is important to remember that you cannot substantiate causal claims from associations alone - behind any causal conslusion there must lie causal assumptions that is **not** testable in observational studies (i.e. correlation does not imply causation). Often we may miss various important piece of information, which leads to confounding information. This is known as [Simpson's Paradox](http://en.wikipedia.org/wiki/Simpson%27s_paradox).

# In the ideal world...

we already know how to solve this problem! We can rephrase this question by simply saying, what we really want to know is the average difference between performing treatment on one _random_ group of people compared to another. In this instance we simply can perform the [`t-test`](http://en.wikipedia.org/wiki/Student%27s_t-test).

However things are not always that simple in practise, after all, a sample of people could have unique traits. For example, in medical trials you are normally conducting trials on volunteers who possess particular traits. How could you then use this information to generalise to the whole population?

## The easy solution

Indeed the easiest way is simply treat these predictors in a regression model, thus following the general principles of regression modelling.

> Insert example here

# Matching

Matching is a statistical technique used to evaluate the effect of a treatment. The goal of matching is, for every treated unit, to find one (or more) non-treated unit(s) with similar observable characteristics against wholm the effect of the treatment can be assessed. Through matching, it enables a comparison of outcomes among treated and non-treated units to estimate the effect of the treatment without reducing bias due to confounding.

## A motivating example

_...from [wikipedia](http://en.wikipedia.org/wiki/Rubin_causal_model)_

Suppose we wanted to know the effects that a drug has on patients, however we can't both run a test case on the patient when they take and don't take the drug, and hence we have imperfect information. Let's say that the drug is for blood pressure, then our data might look like the following:

```
\begin{array} {|r|r|}
\hline
\text{Subject} & \text{Change under treatment} & \text{Change under no treatment} \\
\hline
\text{Joe} & \text{?} & -5 \\
\hline
\text{Bob} & 0 & \text{?} \\
\hline
\text{James} & -50 & \text{?} \\
\hline
\text{Mary} & -10 & \text{?} \\
\hline
\text{Sally} & -20 & \text{?} \\
\hline
\text{Susie} & \text{?} & -20 \\
\hline
\end{array}
```

From the table above, we can then quite simply, assuming that every subject is the same, compute and determine the average causal effect of this drug, we can use the following `R` code to do this:

```r
drug <- as.data.frame(list(change.blood.pressure=c(-5,10,10,-20,5,15),
                           treatment=c(1,1,0,1,0,0)))
ddply(drug, ~ treatment, summarize,
      mean.blood.pressure = mean(blood.pressure))
```

By inspection we would say that the causal effect of taking the drug is a change in blood pressue of \\(-5 - 10 = -15\\).

However we might realise that sex and the starting blood pressure might be important. This is where we would performing matching to perhaps remove any bias.

## Perfectly Matched

Now if we have a dataset which is perfectly matched, we can still create the same conclusions! It would be our goal to somehow create a dataset which looks like the one below:

```
\begin{array} {|r|r|}
\hline
\text{Subject} & \text{Gender} & \text{Blood Pressure} & \text{Change under treatment} & \text{Change under no treatment} \\
\hline
\text{Joe} & \text{male} & 180 & \text{?} & -15 \\
\hline
\text{Bob} & \text{male} & 180 & -20 & \text{?} \\
\hline
\text{James} & \text{male} & 160 & -10 & \text{?} \\
\hline
\text{Paul} & \text{male} & 160 & \text{?} & -5 \\
\hline
\text{Mary} & \text{female} & 180 & 5 & \text{?} \\
\hline
\text{Sally} & \text{female} & 180 & \text{?} & 10 \\
\hline
\text{Susie} & \text{female} & 160 & 5 & \text{?} \\
\hline
\text{Jen} & \text{female} & 160 & \text{?} & 10 \\
\hline
\end{array}
```

Again, since it is perfectly matched, we can then simply take the average the same way that we did it above.

```r
drug <- as.data.frame(list(
  sex = rep(c("m", "f"), each=4),
  blood.pressure = c(180,180,160,160,180,180,160,160),
  change.in.bp = c(-15,-20,-10,-5,5,10,5,10),
  treatment = c(0,1,1,0,1,0,1,0)
  ))

ddply(drug, ~treatment, summarize,
      mean.blood.pressure = mean(blood.pressure),
      mean.change.in.bp = mean(change.in.bp))
```

This would assume that the matched units are homogeneous, and that they have the same causal effect. We could think of it as being we have two similar people, what if one was treated and the other was not. This is the idea behind matching. We must be careful though, because we are **not** trying to ensure that each _pair of matched observations_ is similar in terms of their covariate values, but rather that the matched groups are similar _on average_ across all their covariate values.

```r
library(arm)
data(lalonde)

lalonde$treat <- as.factor(lalonde$treat)

fit <- glm(treat ~ re74 + re75 + age + factor(educ) +
             black + hisp + married + nodegr + u74 + u75,
           family=binomial(link="logit"),
           data=lalonde)

ps.fit.2 <- glm (treat ~ bwg)


pscores <- predict(fit, type="response")


# matchines according to the propensity score as determined in the glm
lalonde.treat <- unlist(Map(function(x) {if(x==1){return(TRUE)} else{return(FALSE)}}, lalonde$treat))
matches <- matching(z = lalonde.treat, score=pscores)
matched <- lalonde[matches$matched,]

b.stats <- balance(lalonde, matched, fit)
print(b.stats)
plot(b.stats)

bm <- data.frame(list(treat = lalonde$treat, pscores = pscores)) # before matching dataset
am <- bm[matches$matched,] # after matching dataset

library(ggplot2)
ggplot(bm, aes(pscores, fill=treat)) + geom_density(alpha=0.2)
ggplot(am, aes(pscores, fill=treat)) + geom_density(alpha=0.2)

#using other library
library(Matching)
rr <- Match(Tr=lalonde.treat, X=pscores)
# we can get all the obs which were matched and retained...
rr.matched <- c(rr$index.treated, rr$index.control) # matches are repeated as necessary for weights??
am1 <- bm[rr.matched,]
ggplot(am1, aes(pscores, fill=treat)) + geom_density(alpha=0.2)

dim(unique(am1))
dim(unique(am))
```

---

[Getting iPython rMagic working on windows](http://www.jedludlow.com/2014/04/getting-ipython-rmagic-to-work-on.html)

1. As described above, pick a suitable flavor of R, and put it on the PATH.
2. Pull down the (unofficial) [rpy2 binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/#rpy2) from Christhoph Gohlke's treasure trove. Make sure you pick the right Python version of the binaries to match your Python flavor, including Python version number.
3. Install those binaries into whatever Python environment you intend to use. (Did you know you can install binary packages into any Python environment, even those not in the Windows registry? That includes an Anaconda install.)
4. Create an environment variable called R_HOME, and point it to the base R install directory. If your paths look like those above, that would be "C:\Program Files\R\R-3.0.3" or whatever matches your R version.
5. Create an environment variable called R_USER, and set it to your Windows username. This looks to be in line with recent discussion on the R development issues list.

To install binaries use the following command.

```
easy_install BINARY.EXE
```

Your user would be the user in

```
C:\User\WINDOWS_USER
```

Also check these [instructions](http://www.swegler.com/becky/blog/2014/08/03/ipython-ipython-notebook-anaconda-and-r-rpy2/) which might demonstrate that
infact Python will need to be downgraded to support this, a solution would be to try this on an admin computer and then transfer it over here (via drive or mega).

---

**this post will only contain information available from the internet at the time of posting and my own opinions. This is not a holistic representative of Mu Sigma's offering but merely my own thoughts.**

Currently I have the pleasure of working with and partnering with [Mu Sigma](http://www.mu-sigma.com/) in my current role. Recently, the focus was on muPDNA; Mu's decision science approach. Here I will break down what it is and my various thoughts around it.

In general there is a cyclical relationship among three areas:

- Design
- Representation
- Hypothesis

# Design

In the _initial_ design phase there really are two loose questions around **objectives**;

- Where are we now? (Current state)
- Where do we want to be? (Future state)

This is an important aspect, after all defining the question is perhaps the most important part of problem solving.

> If I had an hour to solve a problem I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions. - Albert Einstein

These initial questions are important as they allow potential questions to come up sooner, so that rework may be avoided.

# Representation

It is important to not enter a problem with as few assumptions as possible. This is because we are often victims to our own cognitive biases. We may also rely too much on traditions or heuristics; unable to then function when people rightfully challenge our set of beliefs.

> ... most adults wouldn’t answer any question by telling you what their second favourite thing is, but that’s a thing that a child might do all the time. And I think the beauty of thinking like a child, ... is that sometimes doing things differently and simply and with a kind of joy and triviality leads you to a really special place that as an adult you don’t get to go to very often. [Freakonomic's podcast "Think Like a Child"](http://freakonomics.com/2014/05/22/think-like-a-child-full-transcript/)

The key idea in this area is to approach the problem with an open mind, to list out every factor, then every subfactor, and then go to every attribute and so on. This is reminiscent of the Minto method I explored a few posts earlier on continually asking "Why?".

Another reason why I feel this portion is critical is that perhaps when you start listing out problems and their factors you might realise that you actually are asking the wrong question and return to the Design phase - again this approach is to reduce the amount of possible rework that you may possibly have to do and I do feel that it is perhaps effective in that regard.

# Hypothesis

Now that you've listed factors relating to the problem its time to ask questions (or hypothesis), things which you need to explore further and questions which you need to ask. It is to understand what information is available to assess whether the efficacy and efficiency of the problem at hand. It is important to realise that if you confirm a hypothesis, then you have a **measurement**, otherwise you have made a **discovery**.

It is in this portion where we may design tests to answer the hypothesis which we might have, which in turn may be necessary to indeed discover perhaps the gaps and questions which we might have.

References:

- [muPDNA](http://www.mu-sigma.com/analytics/platforms/mupdna/components.html#top)
- [What is muPDNA?](https://www.youtube.com/watch?v=cC8l-7tzl4k)

---

[A Little Book of R for Time Series](http://a-little-book-of-r-for-time-series.readthedocs.org/)

[A Little Book of R for Multivariate Analysis](http://little-book-of-r-for-multivariate-analysis.readthedocs.org/)

---

`caret` package for training models.

---

[Do we Need Hundreds of Classiers to Solve Real World Classication Problems?](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf)

by Manuel Fernadez, Eva Cernadas, Senen Barro from University of Santiago de Compostela.

The classes of the most important classifiers could be:

1. Random forest
2. SVM
3. NNET
4. DTs

---

After mortagage is sorted, here is my savings plan:

Investment into following proportion

- 10% into government bonds
- 70% ASX index
- 20% International index

And hold onto some cash.

---

<http://www.theatlantic.com/magazine/archive/2014/12/the-real-roots-of-midlife-crisis/382235/?single_page=true>

---

Logistic SAS scoring example.

```
data Remission;
   input remiss cell smear infil li blast temp;
   label remiss='Complete Remission';
   datalines;
1   .8   .83  .66  1.9  1.1     .996
1   .9   .36  .32  1.4   .74    .992
0   .8   .88  .7    .8   .176   .982
0  1     .87  .87   .7  1.053   .986
1   .9   .75  .68  1.3   .519   .98
0  1     .65  .65   .6   .519   .982
1   .95  .97  .92  1    1.23    .992
0   .95  .87  .83  1.9  1.354  1.02
0  1     .45  .45   .8   .322   .999
0   .95  .36  .34   .5  0      1.038
0   .85  .39  .33   .7   .279   .988
0   .7   .76  .53  1.2   .146   .982
0   .8   .46  .37   .4   .38   1.006
0   .2   .39  .08   .8   .114   .99
0  1     .9   .9   1.1  1.037   .99
1  1     .84  .84  1.9  2.064  1.02
0   .65  .42  .27   .5   .114  1.014
0  1     .75  .75  1    1.322  1.004
0   .5   .44  .22   .6   .114   .99
1  1     .63  .63  1.1  1.072   .986
0  1     .33  .33   .4   .176  1.01
0   .9   .93  .84   .6  1.591  1.02
1  1     .58  .58  1     .531  1.002
0   .95  .32  .3   1.6   .886   .988
1  1     .6   .6   1.7   .964   .99
1  1     .69  .69   .9   .398   .986
0  1     .73  .73   .7   .398   .986
;
run;

data remis1;
   input cell smear infil li blast temp;
   datalines;
 .8   .83  .66  1.9  1.1     .996
 .9   .36  .32  1.4   .74    .992
 .8   .88  .7    .8   .176   .982
 .95  .36  .34   .5  0      1.038
 .85  .39  .33   .7   .279   .988
 .7   .76  .53  1.2   .146   .982
 .8   .46  .37   .4   .38   1.006
 .2   .39  .08   .8   .114   .99
1     .9   .9   1.1  1.037   .99
1     .58  .58  1     .531  1.002
 .95  .32  .3   1.6   .886   .988
1     .6   .6   1.7   .964   .99
1     .69  .69   .9   .398   .986
1     .73  .73   .7   .398   .986
;
run;

proc logistic data=Remission outest=betas covout;
   model remiss(event='1')=cell smear infil li blast temp
                / selection=stepwise
                  slentry=0.3
                  slstay=0.35
                  details
                  lackfit;
   output out=pred p=phat lower=lcl upper=ucl
          predprob=(individual crossvalidate);
	score data=remis1 out=score_r;
run;
```

---

[Category Theory](http://math.stackexchange.com/questions/370/good-books-and-lecture-notes-about-category-theory)

Consider the books:

- Topoi: the categorial analysis of logic
- Categories for the working mathematician
- [Basic Category Theory (in progress)](http://www.maths.ed.ac.uk/~tl/bct/)

---

From [stackoverflow](http://stackoverflow.com/questions/44965/what-is-a-monad).

First: The term **monad** is a bit vacuous if you are not a mathematician. An alternative term is **computation builder** which is a bit more descriptive of what they are actually useful for.

You ask for practical examples:

**Example 1: List comprehension**:

    [x*2 | x<-[1..10], odd x]

This expression returns the doubles of all odd numbers in the range from 1 to 10. Very useful!

It turns out this is really just syntactic sugar for some operations within the List monad. The same list comprehension can be written as:

    do
       x <- [1..10]
       if odd x
           then [x * 2]
           else []

Or even:

    [1..10] >>= (\x -> if odd x then [x*2] else [])

**Example 2: Input/Output**:

    do
       putStrLn "What is your name?"
       name <- getLine
       putStrLn ("Welcome, " ++ name ++ "!")

Both examples uses monads, aka computation builders. The common theme is that the monad _chains operations_ in some specific, useful way. In the list comprehension, the operations are chained such that if an operation returns a list, then the following operations are performed on _every item_ in the list. The IO monad on the other hand performs the operations sequentially, but passes a "hidden variable" along, which represents "the state of the world", which allows us to write IO code in a pure functional manner.

It turns out the pattern of _chaining operations_ is quite useful, and is used for lots of different things in Haskell.

Another example is exceptions: Using the `Error` monad, operations are chained such that they are performed sequentially, except if an error is thrown, in which case the rest of the chain is abandoned.

Both the list-comprehension syntax and the do-notation are syntactic sugar for chaining operations using the `>>=` operator. A monad is basically just a type that supports the `>>=` operator.

**Example 3: A parser**

This is a very simple parser which parses either a quoted string or a number:

    parseExpr = parseString <|> parseNumber

    parseString = do
            char '"'
            x <- many (noneOf "\"")
            char '"'
            return (StringValue x)

    parseNumber = do
        num <- many1 digit
        return (NumberValue (read num))

The operations `char`, `digit` etc. are pretty simple, they either match or don't match. The magic is the monad which manages the control flow: The operations are performed sequentially until a match fail, in which case the monad backtracks to the latest `<|>` and tries the next option. Again, a way of chaining operations with some additional, useful semantics.

**Example 4: Asynchronous programming**

The above examples are in Haskell, but it turns out F# also supports monads. This example is stolen from [Don Syme](http://blogs.msdn.com/dsyme/archive/2007/10/11/introducing-f-asynchronous-workflows.aspx):

    let AsyncHttp(url:string) =
        async {  let req = WebRequest.Create(url)
                 let! rsp = req.GetResponseAsync()
                 use stream = rsp.GetResponseStream()
                 use reader = new System.IO.StreamReader(stream)
                 return reader.ReadToEnd() }

This method fetches a web page. The punch line is the use of `GetResponseAsync` - it actually waits for the response on a separate thread, while the main thread returns from the function. The last three lines are executed on the spawned thread when the response have been received.

In most other languages you would have to explicitly create a separate function for the lines that handle the response. The `async` monad is able to "split" the block on its own and postpone the execution of the latter half. (The `async {}` syntax indicates that the control flow in the block is defined by the `async` monad)

**How they work**

So how can a monad do all these fancy control-flow thing? What actually happens in a do-block (or a _computation expression_ as they are called in F#), is that every operation (basically every line) is wrapped in a separate anonymous function. These functions are then combined using the `bind` operator (spelled `>>=` in Haskell). Since the `bind` operation combines functions, it can execute them as it sees fit: sequentially, multiple times, in reverse, discard some, execute some on a separate thread when it feels like it and so on.

As an example, this is the expanded version of the IO-code from example 2:

    putStrLn "What is your name?"
    >>= (\_ -> getLine)
    >>= (\name -> putStrLn ("Welcome, " ++ name ++ "!"))

This is uglier, but it's also more obvious what is actually going on. The `>>=` operator is the magic ingredient: It takes a value (on the left side) and combines it with a function (on the right side), to produce a new value. This new value is then taken by the next `>>=` operator and again combined with a function to produce a new value. `>>=` can be viewed as a mini-evaluator.

Note that `>>=` is overloaded for different types, so every monad has its own implementation of `>>=`. (All the operations in the chain have to be of the type of the same monad though, otherwise the `>>=` operator wont work.)

The simplest possible implementation of `>>=` just takes the value on the left and applies it to the function on the right and returns the result, but as said before, what makes the whole pattern useful is when there is something extra going on in the monad's implementation of `>>=`.

There is some additional cleverness in how the values are passed from one operation to the next, but this requires a deeper explanation of the Haskell type system.

**Summing up**

In Haskell-terms a monad is a parameterized type which is an instance of the Monad type class, which defines &gt;&gt;= along with a few other operators. In layman's terms, a monad is just a type for which the `>>=` operation is defined.

In itself `>>=` is just a cumbersome way of chaining functions, but with the presence of the do-notation which hides the "plumbing", the monadic operations turns out to be a very nice and useful abstraction, useful many places in the language, and useful for creating your own mini-languages in the language.

**Why are monads hard?**

For many Haskell-learners, monads are an obstacle they hit like a brick wall. It's not that monads themselves are complex, but that the implementation relies on many other advanced Haskell features like parameterized types, type classes, and so on. The problem is that Haskell IO is based on monads, and IO is probably one of the first things you want to understand when learning a new language - after all, its not much fun to create programs which don't produce any output. I have no immediate solution for this chicken-and-egg problem, except treating IO like "magic happens here" until you have enough experience with other parts of language. Sorry.

---

Solutions to Haskell (FP) homework (lab 3):

```
module Lab3 where

-----------------------------------------------------------------------------------------------------------------------------
-- LIST COMPREHENSIONS
------------------------------------------------------------------------------------------------------------------------------

-- ===================================
-- Ex. 0 - 2
-- ===================================

evens :: [Integer] -> [Integer]
evens = filter even

-- ===================================
-- Ex. 3 - 4
-- ===================================

-- complete the following line with the correct type signature for this function
squares :: Integer -> [Integer]
squares n = map (\x -> x * x) [1 .. n]

sumSquares :: Integer -> Integer
sumSquares n = sum (squares n)

-- ===================================
-- Ex. 5 - 7
-- ===================================

-- complete the following line with the correct type signature for this function
squares' :: Integer -> Integer -> [Integer]
squares' m n = drop (fromIntegral n) (squares (m+n))

sumSquares' :: Integer -> Integer
sumSquares' x = sum . uncurry squares' $ (x, x)

-- ===================================
-- Ex. 8
-- ===================================

coords :: Integer -> Integer -> [(Integer,Integer)]
coords m n = [(x,y) | x <- [0..m], y <- [0..n]]




```

---

Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.

Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.

---

Solutions to Haskell (FP) homework (lab 2):

```
-- ===================================
-- Ex. 0
-- ===================================

toDigits :: Integer -> [Integer]
toDigits x = map (\y -> read [y]::Integer) (show x)

-- eval xs = foldl (\x y -> y + (10 * x)) 0 xs

-- ===================================
-- Ex. 1
-- ===================================

toDigitsRev :: Integer -> [Integer]
toDigitsRev x = reverse (toDigits x)

-- ===================================
-- Ex. 2
-- ===================================

doubleSecond :: [Integer] -> [Integer]
doubleSecond xs = map (\(x,y) -> x * ((y `mod` 2) + 1) ) (zip xs [0 .. ])


-- ===================================
-- Ex. 3
-- ===================================

sumDigits :: [Integer] -> Integer
sumDigits xs = sum (map (\x -> (sum . toDigits) x) xs)


-- ===================================
-- Ex. 4
-- ===================================

isValid :: Integer -> Bool
isValid xs = (sumDigits . doubleSecond . toDigitsRev) xs `mod` 10 == 0



-- ===================================
-- Ex. 5
-- ===================================

numValid :: [Integer] -> Integer
numValid xs = sum . map (\_ -> 1) $ filter isValid xs
```

---

How to "autobuild" pages for github:

It is all in the `./.git/config` file. Simply add:

```
  push = +refs/heads/master:refs/heads/gh-pages
  push = +refs/heads/master:refs/heads/master
```

which will then link `gh-pages` and `master` together. Also make sure that you change the `url` to `git@github.com` so that it uses SSH, allowing you to fully automate it by linking to your SSH key(s).

---

Semantic sorting solutions

**Python**

```
sample_input = """2.0.11-alpha
0.1.7+amd64
0.10.7+20141005
2.0.12+i386
1.2.34
2.0.11+i386
20.1.1+i386""".split('\n')

# one line solution
#sample_input.sort(key=lambda x: (x.split('.')[0],x.split('.')[1],x.split('.')[2].split('-')[0]))

import re

def parseData(semantic):
    """takes in sematic version and parses into the various parts"""
    # remove metadata
    semantic_ = re.sub(r'\+\w+$','', semantic)
    major, minor, patchlabel = semantic_.split('.')
    try:
        patch, label = patchlabel.split('-')
        label = -1
    except:
        patch = patchlabel
        label = 0
    return (semantic, int(major), int(minor), int(patch), label)

si = sorted([parseData(x) for x in sample_input],
             key=lambda x: (x[1], x[2], x[3], x[4]))
print '\n'.join([x[0] for x in si])
```

**R**

```
require(magrittr)

sample_input = "2.0.11-alpha
0.1.7+amd64
0.10.7+20141005
2.0.12+i386
1.2.34
2.0.11+i386
20.1.1+i386"

sample_input = strsplit(sample_input, "\n") %>% unlist

parse_data <- function(semantic) {
  semantic1 = sub("\\+\\w+$", "", semantic)

  # break the string into three components
  s.split = strsplit(semantic1, "\\.") %>% unlist
  major = s.split[1] %>% as.numeric
  minor = s.split[2] %>% as.numeric
  patchlabel = strsplit(s.split[3], "-") %>% unlist
  if (length(patchlabel) > 1){
    patch = patchlabel[1] %>% as.numeric
    label = -1
  } else {
    patch = patchlabel %>% as.numeric
    label = 0
  }

  return(c(semantic, major, minor, patch, label))
}

output = sapply(sample_input, parse_data) %>% t %>% as.data.frame
output$V2 %<>% unlist %>% as.numeric
output$V3 %<>% unlist %>% as.numeric
output$V4 %<>% unlist %>% as.numeric
output$V5 %<>% unlist %>% as.numeric
arrange(output, V2, V3, V4, V5)$V1 %>% unlist %>% as.character
```

**JavaScript**

```
var sample_input = "2.0.11-alpha,0.1.7+amd64,0.10.7+20141005,2.0.12+i386,1.2.34,2.0.11+i386,20.1.1+i386".split(',');

output = _.map(sample_input, function(s) {

  // create an object that we can use on it
  sobject = {};
  sobject.name = s;

  sgroup = s.replace(/\+\w+$/,"").split('.');
  sobject.major = parseInt(sgroup[0]);
  sobject.minor = parseInt(sgroup[1]);

  patchlabel = sgroup[2].split('-');
  if(patchlabel.length == 2) {
    sobject.patch = patchlabel[0];
    sobject.label = -1;
  } else {
    sobject.patch = patchlabel;
    sobject.label = 0;
  }

  return(sobject);
});

function compare(a,b) {
  if (a.major < b.major) {
    return -1;
  }
  if (b.major < a.major) {
    return 1;
  }
  if (a.minor < b.minor) {
    return -1;
  }
  if (b.minor < a.minor) {
    return 1;
  }
  if (a.patch < b.patch) {
    return -1;
  }
  if (b.patch < a.patch) {
    return 1;
  }
  if (a.label < b.label) {
    return -1;
  }
  if (b.label < a.label) {
    return 1;
  }
  return 0;
}

output = output.sort(compare);
output = _.map(output, function(a) {return a.name;});
console.log(output);
```

---

Installing `nltk` data offline:

1. Download the [nltk-data.zip file](http://sourceforge.net/projects/nltk/files/).
2. Uncompress into the relevant directory. The file structure should look like the below:

```
Folder PATH listing
Volume serial number is 00000002 2902:B6FE
~PYTHON\LIB\NLTK_DATA
├───chunkers
├───corpora
│   ├───abc
│   ├───alpino
│   ├───biocreative_ppi
│   ├───brown
│   ├───cess_cat
│   ├───cess_esp
│   ├───chat80
│   ├───cmudict
│   ├───conll2000
│   ├───conll2002
│   ├───floresta
│   ├───gazetteers
│   ├───genesis
│   ├───gutenberg
│   ├───ieer
│   ├───inaugural
│   ├───indian
│   ├───kimmo
│   ├───nps_chat
│   ├───webtext
│   └───treebank
│       ├───combined
│       ├───parsed
│       ├───raw
│       └───tagged
├───grammars
├───samples
├───stemmers
│   └───rslp
├───taggers
└───tokenizers
    └───punkt
```

---

Using `ttk` in Python 2.7

```python
from Tkinter import Tk, StringVar
import ttk

def calculate(*args):
    try:
        value = float(feet.get())
        meters.set('%g' % (0.3048 * value))
    except ValueError:
        if not feet.get():
            meters.set('')

root = Tk()
root.title("Feet to Meters")

feet = StringVar()
feet.trace_variable('w', calculate)
meters = StringVar()

main = ttk.Frame(root)
main.grid(sticky='nsew')

ttk.Label(main, text="Feet:").grid(row=0, sticky='e')
feet_entry = ttk.Entry(main, width=7, textvariable=feet)
feet_entry.grid(row=0, column=1, sticky='ew')
feet_entry.focus()

ttk.Label(main, text="Meters:").grid(row=1, sticky='e')
ttk.Label(main, textvariable=meters).grid(row=1, column=1, sticky='ew')

root.mainloop()



```

---

[Resume/CV Driven Developing](http://radar.oreilly.com/2014/10/resume-driven-development.html) - on hiring skill sets and developing teams

---

getting unix tools working on windows is closest to magic as you can get. Heres the "grep" recursive include statement that works with GOW

```
grep -r --include="*.txt" STRING . -l
```

---

[John Carmack on Inline Code](http://number-none.com/blow/john_carmack_on_inlined_code.html)

---

Draggable with coord (pure js)

```
var drags = document.querySelectorAll('#drag');
[].forEach.call(drags, function(drag) {
  drag.setAttribute("draggable", "true");
  drag.addEventListener('dragstart', handleDragStart, false);
  drag.addEventListener('dragend', handleDragEnd, false);
});

function handleDragStart(e){
  console.log('handleDragStart');
  console.log('start coord: '+e.clientX + " " + e.clientY);
}

function handleDragEnd(e){
  console.log('handleDragEnd');
  console.log('end coord: '+e.clientX + " " + e.clientY);
}
```

create some item with id drag...

---

[clock angle](http://problemotd.com/problem/clock-angle/)

```
def clock_angle(hr, minute):
    hr_ = (hr % 12) * (360/12)
    min_ = (minute % 60) * (360/60)
    return float(min_- hr_) - ((float(minute)/60)*(360/12))

print clock_angle(6, 15)
print clock_angle(12, 0)
print clock_angle(12, 15)
print clock_angle(12, 5)
print clock_angle(3, 0)
```

---

Reminder on how to build exe using srlua...a batch script like this would do:

```
set PATH=%PATH%;H:\My Documents\03_Languages\Lua;H:\My Documents\03_Languages\srlua

:: copy the dll to this folder too!
xcopy "H:\My Documents\03_Languages\srlua\lua52.dll" .


:: edit below this line
glue.exe "H:\My Documents\03_Languages\srlua\srlua.exe" hello.lua hello.exe
```

---

Easy Gui using [Gooey](https://github.com/chriskiehl/Gooey)

---

[Don't follow your dreams](http://www.fortressofdoors.com/follow-your-dreams-or-maybe-dont/)

---

[daily programmer](http://www.reddit.com/r/dailyprogrammer/comments/2d8yk5/8112014_challenge_175_easy_bogo/)

    -- bogo sort

    -- "sorts" a vector
    function bsort(a)
      for i=1, (#a*2) do
        x1 = math.random(#a)
        x2 = math.random(#a)
        a[x1], a[x2] = a[x2], a[x1]
      end
      return(a)
    end

    function bsortstr(orig)
      torig = {}
      orig:gsub(".", function(c) table.insert(torig, c) end)
      torig = bsort(torig)
      return(table.concat(torig, ''))
    end

    function bogo(orig, target)
      local i = 0
      while (string.lower(orig) ~= string.lower(target)) do
        i = i+1
        orig = bsortstr(orig)
      end
      return(i)
    end

    print(bogo("asfd", "asdf"))

---

[Why we procrastinate](http://nautil.us/issue/16/nothingness/why-we-procrastinate)

The British philosopher Derek Parfit espoused a severely reductionist view of personal identity in his seminal book, Reasons and Persons: It does not exist, at least not in the way we usually consider it. We humans, Parfit argued, are not a consistent identity moving through time, but a chain of successive selves, each tangentially linked to, and yet distinct from, the previous and subsequent ones. The boy who begins to smoke despite knowing that he may suffer from the habit decades later should not be judged harshly: “This boy does not identify with his future self,” Parfit wrote. “His attitude towards this future self is in some ways like his attitude to other people.”

...

Hershfield has taken a more high-tech approach. Inspired by the use of images to spur charitable donations, he and colleagues took subjects into a virtual reality room and asked them to look into a mirror. The subjects saw either their current self, or a digitally aged image of themselves (see the figure, Digital Old Age). When they exited the room, they were asked how they’d spend $1,000. Those exposed to the aged photo said they’d put twice as much into a retirement account as those who saw themselves unaged.

---

[daily programmer](http://www.reddit.com/r/dailyprogrammer/comments/2cld8m/8042014_challenge_174_easy_thuemorse_sequences/)

    library(magrittr)
    n=10
    eval(parse(text=paste(c(0,rep(" %>% c(.,1-.) ", n)), collapse='')))

---

[Day9 on being a better person](http://np.reddit.com/r/IAmA/comments/2cwxcf/i_am_sean_day9_plott_spellslingers_host_esports/cjjvr1b?context=3)

If you surf through some of my old posts on forums, you'll see an angry little kid. I'm super embarrassed sometimes when I see the sorts of things I used to do. Gaaaaagh.

But, I've always wanted to be nice. I like to be nice and to help out. But how does one "work" on being nice? Here's my routine:

When I wake up in the morning, I think about all the people I'm going to talk to and interact with that day. I think of how a conversation might go so I have an expectation, and I try to think of how I WANT to come across in that interaction. I think of all the things that people could be frustrated at me about, how they might say it to me, and how I can make that experience NICE for the other person. I literally rehearse in my head various situations until it sounds right.
Suppose I'm going into a performance review with a boss later that day and I think he might be angry at my recent work. I'll try to rehearse exactly what I want to say to make the experience nice for HIM. How would I posture my body language? What words would I use? How would I repair this situation? After a while of talking to myself, some possible ideas don't "feel right" and others feel really good.

The feeling I look for is one of calm and relief. Ever gotten into an argument with someone in your head? Feel your face getting hot and your heartrate increasing? That's bad. Don't say those in real life. I always feel a little mental "click" when I've thought of the right thing to say. It feels warm and I feel like it'll be nice for the other person.
At the end of a day, I'll reflect on how those interactions went. In particular, I'll reflect first on the ones that felt off - the ones where I felt a bit hot in the face. How could I have said something differently? How could I have communicated more calmly to this person? How could I have helped this person feel more involved? I just continue to reflect until I feel that "click."

Most importantly, I take time every few days to think about all the rather mundane, dry, utterly forgetful moments and conversations throughout the day. That I think is the real trick. It's so easy to "feel fine" during a conversation and to be unaware if it's negatively affecting anyone. That's where I've found little habits I do that may be offputting, like looking away from someone as they tell a joke, laughing into the distance. Fuck that, if I'm gonna laugh, I now try to look RIGHT at the person so they FEEL my enjoyment.

After years of reflecting, adjusting, rehearsing, you just become more cheerful and happy. It took a hell of a lot of work and reflection, and I'm sure I grate on people and get snippy, but it's something I think a lot about.
I worry about someone having a struggle in a day, and I say the one thing that pushes it into a "bad day." Everyone has immense struggles in the own life and I don't want to add to it. I want it to be better!

---

CC5 Speech: On Giving up Dreams (or why my 19yr old self is a moron) "Bad Arguments"

Logical Fallacies/Cognitive Biases:

1. Appeal to Fear: "Imagine a scary future". E.g. "You should give me all your valuables before the police get here.
   They will end up putting them in a storeroom and things tend to get lost in the storeroom"
2. No True Scotsman: A scotsman once read an article in the paper about an Englishman who committed a heinous crime,
   and exclaims "No Scotsman would do such a thing." the next day, he comes across an article about a Scotsman
   committing an even worse crime, to which instead of amending his claim, he reacts by saying "No true Scotsman would do such a thing.
3. Sunk Cost Fallacy: Continually invest in something that is hopeless and doomed to fail.

---

[Key word search using CMU sphinx (specifically pocket sphinx)](http://sourceforge.net/p/cmusphinx/discussion/help/thread/8b6cca9d/?limit=25)

---

On Relative Deprivation, [why perhaps you shouldn't go to the best school you can get into EICD (Elite Institute Cognitive Disorder)](https://www.youtube.com/watch?v=3UEwbRWFZVc).

---

[Opinion: The unspoken truth about managing geeks](http://www.computerworld.com/s/article/print/9137708/Opinion_The_unspoken_truth_about_managing_geeks?taxonomyName=Management&taxonomyId=14)

---

[Norris Numbers](http://www.teamten.com/lawrence/writings/norris-numbers.html), walls you hit in program size.

Reading [comments](http://www.reddit.com/r/programming/comments/2bgm0x/walls_you_hit_in_program_size/) really hit home about how I should really start writing Scala.

---

On constraints and design...

> Mme. L. Amic: What is your definition of “Design”, Monsieur Eames?
> Charles Eames:One could describe design as a plan for arranging elements to accomplish a particular purpose.
>
> Mme. L. Amic: Is Design an expression of art?
> Charles Eames:I would rather say it’s an expression of purpose. It may, if it’s good enough, later be judged as art.
>
> Mme. L. Amic: Is Design a craft for industrial purposes?
> Charles Eames:No, but design may be a solution to some industrial problems.
>
> Mme. L. Amic: What is the boundaries of Design?
> Charles Eames:What are the boundaries of problems?
>
> Mme. L. Amic: Is Design a discipline that concerns itself with only one part of the environment?
> Charles Eames:No.
>
> Mme. L. Amic: Is it a method of general expression?
> Charles Eames:No, it is a method of action.
>
> Mme. L. Amic: Is Design a creation of an individual?
> Charles Eames:No, because to be realistic one must always recognize the influence of those that have gone before.
>
> Mme. L. Amic: Is Design a creation of a group?
> Charles Eames:Very often.
>
> Mme. L. Amic: Is there a Design ethic?
> Charles Eames:There are always design constraints and these often imply an ethic.
>
> Mme. L. Amic: Does Design imply the idea of products that are necessarily useful?
> Charles Eames:Yes, even though the use might be very suttle.
>
> Mme. L. Amic: Is it able to cooperate in the creation of works reserved solely for pleasure?
> Charles Eames:Who would say that pleasure is not useful?
>
> Mme. L. Amic: Ought form to derive from the analysis of function?
> Charles Eames:The great risk here is that the analysis may be incomplete.
>
> Mme. L. Amic: Can the computer substitute for the Designer?
> Charles Eames:Probably, in some special cases but usually the computer is an aid to the designer.
>
> Mme. L. Amic: Does Design imply industrial manufacture?
> Charles Eames:Not neccessarily.
>
> Mme. L. Amic: Is Design used to modify an old object through new techniques?
> Charles Eames:This is one kind of design problem.
>
> Mme. L. Amic: Is Design used to fit up an existing model so that it is more attractive?
> Charles Eames:One doesn’t usually think of design in this way.
>
> Mme. L. Amic: Is Design an element of industrial policy?
> Charles Eames: If design constraints imply an ethic and if industrial policy includes ethical principles then yes, design is an element in industrial policy.
>
> Mme. L. Amic: Does the creation of Design admit constraint?
> Charles Eames:Design depends largely on constraints.
>
> Mme. L. Amic: What constraints?
> Charles Eames:The sum of all constraints. Here is one of the few effective keys to the design problem: the ability of the designer to recognize as many of the constraints as possible, his willingness and enthusiasm for working within these constraints. The constraints of price, size, strength, balance, time and so forth. Each problem has its own peculiar list.
>
> Mme. L. Amic: Does Design obey laws?
> Charles Eames:Aren’t constraints enough?
>
> Mme. L. Amic: Are there tendencies and schools in Design?
> Charles Eames:Yes, but these are more a measure of human limitation than of ideals.
>
> Mme. L. Amic: Is Design ephemeral?
> Charles Eames:Some needs are ephemeral, most designs are ephemeral.
>
> Mme. L. Amic: Ought Design to tend towards the ephemeral or towards permanence?
> Charles Eames:Those needs and designs that have a more universal quality tend toward relative permanence.
>
> Mme. L. Amic: How would you define yourself with respect to a decorator? An interior architect? A stylist?
> Charles Eames:I wouldn’t.
>
> Mme. L. Amic: To whom does Design address itself: to the greatest numbers? To the specialsts or the enlightened amateur? To a priviledged social class?
> Charles Eames:Design addresses itself to the need.
>
> Mme. L. Amic: After having answered all these questions, do you feel you have been able to practice the profession of “Design” under satisfactory conditions, or even optimum conditions?
> Charles Eames:Yes.
>
> Mme. L. Amic: Have you been forced to accept compromises?
> Charles Eames:I don’t remember ever being forced to accept compromises but I have willingly accepted constraints.
>
> Mme. L. Amic: What do you feel is the primary condition for the practice of Design and for its propagation?
> Charles Eames:A recognition of need.
>
> Mme. L. Amic: What is the future of Design?

[Charles Eames](http://blog.designersrevolt.com/post/52143068880/the-definition-of-design-in-the-words-of-charles-eames) on constraints and design.

---

> I divide my officers into four groups. There are clever, diligent, stupid, and lazy officers. Usually two characteristics are combined. Some are clever and diligent -- their place is the General Staff. The next lot are stupid and lazy -- they make up 90 percent of every army and are suited to routine duties. Anyone who is both clever and lazy is qualified for the highest leadership duties, because he possesses the intellectual clarity and the composure necessary for difficult decisions. One must beware of anyone who is stupid and diligent -- he must not be entrusted with any responsibility because he will always cause only mischief.
> [Kurt von Hammerstein-Equord](http://en.wikipedia.org/wiki/Kurt_von_Hammerstein-Equord)

---

[Goodhart's law](http://en.wikipedia.org/wiki/Goodhart's_law): "When a measure becomes a target, it ceases to be a good measure."

---

[Mathematics is Programming](http://programmers.stackexchange.com/questions/136987/what-does-mathematics-have-to-do-with-programming)

Discusses the thoughts on the Cargo Cult of programming and the cargo cult of mathematics. What is mathematics really?

[Jeremy Kun's respons to Programming is not Maths](http://j2kun.svbtle.com/programming-is-not-math-huh)

---

To get CUDA code running, make sure:

1. `Build Customizations` is set to the CUDA target
2. File extension is `.cu` not `.cpp`
3. The "Item type" in the properties of the `.cu` file is set to `CUDA C/C++`.

---

When you think about it, higher efficiency (whether in workflow processes or software)
leads to lower flexibility. Efficiency is by definition rigid.

This means we should take care when applying ways to encourage working more efficiently;
how much flexibility are we sacrificing; and more importantly, is that flexibility actually
necessary?

---

# Business School

Global MBA from [London School of Business and Finance costs 8500 GBP](http://www.lsbf.org.uk/students/fees-funding.html) or roughly $16k AUD. Though it is not a reputable university.

Looking only at accredited MBAs from Association of MBAs, we have:

- [Robert Gordon University Aberdeen, ~13000 GBP or 24k AUD](http://www.rgu.ac.uk/business-management-and-accounting/study-options/distance-and-flexible-learning/master-of-business-administration).
- [Bradford University School of Management, ~13500 GBP or 25k AUD](http://www.brad.ac.uk/management/programmes/mba/distance-learning-mba/distance-learning-mba/)
- [Warwick Business School, ~22500 GBP or 41k AUD](http://www.wbs.ac.uk/courses/mba/distance-learning/fees/)
- [Oxford Brooks University, ~14000 GBP or 26k](http://business.brookes.ac.uk/mba/fees/)

As a point of comparison looking at Open Universities [RMIT costs over $43k](http://www.rmit.edu.au/browse;ID=ux5sppf6w1ct), and [Melbourne Business School is over $70k](http://mbs.edu/programs/mbaparttime/Pages/Part-Time%20MBA.aspx).

---

# MOOCs and certifications

## [HBX](http://hbx.hbs.edu/)

CORe (Credential of Readiness) is being piloted at Harvard Business School! It would be best to keep an eye out on what is happening in this space.

## [Nanodegrees](https://www.udacity.com/nanodegrees)

Have four particular fields:

- Front-end web developer
- iOS developer
- Back-end web developer

---

# [How the Other Half Works: an Adventure in the Low Status of Software Engineers](https://michaelochurch.wordpress.com/2014/07/13/how-the-other-half-works-an-adventure-in-the-low-status-of-software-engineers/)

Very long article, perhaps worth a read. It is about the difference in value (anecdotally) between an engineer and a manager. I think there is an element of reciprocity in analytical/development work, you have to push back otherwise you won’t get respect.

The gist of it is, when you work in development you are judged on what you don’t know. But when you work in management, they give you the benefit of the doubt. The article phrases it like, in the first instance, everyone has 90 points, but for everything you’re lacking you get negative points, but for the latter, everyone may start on 70, and everything that looks good you would get positive points.

> This whole issue is about more than what one knows and doesn’t know about technology. As programmers, we’re used to picking up new skills. It’s something we’re good at (even if penny-shaving businessmen hate the idea of training us). This is all about social status, and why status is so fucking important when one is playing the work game– far more important than being loyal or competent or dedicated.
>
> Low and high status aren’t about being liked or disliked. Some people are liked but have low status, and some people are disliked but retain high status. In general, it’s more useful and important to have high status at work than to be well-liked. It’s obviously best to have both, but well-liked low-status people get crap projects and never advance. Disliked high-status people, at worst, get severance. As Machiavelli said, “it is far safer to be feared than loved if you cannot be both.” People’s likes and dislikes change with the seasons, but a high-status person is more unlikely to have others act against his interests.
>
> Moreover, if you have low social status, people will eventually find reasons to dislike you unless you continually sacrifice yourself in order to be liked, and even that strategy runs out of time. At high social status, they’ll find reasons to like you. At low status, your flaws are given prime focus and your assets, while acknowledged, dismissed as unimportant or countered with “yes, buts” which turn any positive trait into a negative. (“Yes, he’s good in Clojure, but he’s might be one of those dynamic-typing cowboy coders!” “Yes, he’s good in Haskell, but that means he’s one of those static-typing hard-asses.” “Yes, he’s a good programmer, but he doesn’t seem like a team player.”) When you have low status, your best strategy is to be invisible and unremarkable, because even good distinctions will hurt you. You want to keep your slate ultra-clean and wait for mean-reversion to drift you into middling status, at which point being well-liked can assist you and, over some time– and it happens glacially– bring you upper-middle or high status.

---

# [What Could Possibly Be Worse Than Failure?](http://thedailywtf.com/Articles/What_Could_Possibly_Be_Worse_Than_Failure_0x3f_.aspx)

> So where does this leave us? Do we actually have to wait until a system’s end-of-life before
> we can call our development work a success? Of course we do! We can, however, deem it a failure
> much sooner than that. But only if we’re willing to recognize that we failed.
>
> ...
>
> The path that I was headed down was the same path that the developers of this huge monstrosity had taken before me. They considered every project that they had ever completed to be a success. They weren’t arrogant (at first) – it simply was how they, and everyone else, defined success: making it to production.
>
> Instead of realizing that the first applications they wrote were poorly designed, poorly programmed, and soon-to-be failure – like everyone’s first are – they chalked them off as a success and moved on to the next project. They developed more software. Bigger software. More expensive software. Wrong software. All because they kept thinking that the completion of the project meant that it was a success.
>
> Had they simply admitted to earlier failure – and learned from it – they would have never created such an epic disaster. By imagining that their past projects were a success, they had done far worse than failure.

---

# [The main trick in Machine Learning](http://edinburghhacklab.com/2013/12/the-main-trick-in-machine-learning/)

_from [Edinburgh Hacklab](http://edinburghhacklab.com/2013/12/the-main-trick-in-machine-learning/)_

I have been irritated that many recent introductions to machine learning/neural networks/whatever that fail to emphasise the most import trick in machine learning. Many internet resources don't mention it, and even good textbooks often don't drill it in to the reader the absolute criticality to success the trick is. In a machine learning context, we wish a learning system to generalise. That is, make good predictions on data it has never encounter before, based on what it learnt during from a training set. There is no easy formula to predict the ability of a learning system to **generalise**, but you can estimate it using held out data. That held out data is labelled but it is not used in training. It is called the validation set.

The validation set **is** the main trick. With a validation set in hand, you ask a learning system to make predictions on data you already know the answers to. That gives you a bound of how good the system will be **in general**.

The importance of estimating a learning systems ability to generalise was not obvious to the early pioneers. Vladimir Vapnik (the V in VC dimension), somewhat sarcastically describes the mindset of the 1970s applied learning community in the following excerpt from "The Nature of Statistical Learning Theory"

> "To get good generalization it is sufficient to choose the coefficients ... that provide the minimal number of training errors. The principle of minimizing the number of training errors is a self-inductive principle, and from the practical point of view does not need justification. The main goal of applied analysis is to ... minimize the number of errors on the training data"

---

[Comments](https://news.ycombinator.com/item?id=6877567)

> A professor of mine stated it very well. If you can imagine that there is a _true_ model somewhere out in infinitely large model space then ML is just the search for that model.
>
> In order to make it tractable, you pick a finite model space, train it on finite data, and use a finite algorithm to find the best choice inside of that space. That means you can fail in three ways---you can over-constrain your model space so that the true model cannot be found, you can underpower your search so that you have less an ability to discern the best model in your chosen model space, and you can terminate your search early and fail to reach that point entirely.
>
> Almost all error in ML can be seen nicely in this model. In particular here, those who do not remember to optimize validation accuracy are often making their model space so large (overfitting) at the cost of having too little data to power the search within it.
>
> Devroye, Gyorfi, and Lugosi [A Probabilistic Theory of Pattern Recognition](http://www.amazon.com/Probabilistic-Recognition-Stochastic-Modelling-Probability/dp/0387946187) have a really great picture of this in their book.

---

# Make a Point, Tell a Story

One of the most effective ways to deliever a speech is to do one of the following:

- Make a point, then tell a story
- Tell a story, then make a point

---

# Future Learnings 2014

Now that 2013 is coming to an end (I know this doesn't have dates). I thought I'll compile a list
of material which I want to get through for the next year. (Perhaps not all of it, since its just a random list)

Online Books:

- [A Programmer's Guide to Data Mining](http://guidetodatamining.com/)
- [Probabilistic Programming and Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)
- [Modern Perl](http://modernperlbooks.com/books/modern_perl/chapter_00.html)
- [Scala School](http://twitter.github.io/scala_school/)
- [Real World Haskell](http://book.realworldhaskell.org/)
- [Real World OCaml](https://realworldocaml.org/)

Online Courses:

- [XSeries](https://www.edx.org/xseries)
- [Data Science and Big Data Track](http://blog.udacity.com/2013/11/sebastian-thrun-launching-our-data.html)
- [Masters of Science - Georgia Tech](http://www.omscs.gatech.edu/)

Books

- [Scala for the Impatient](http://horstmann.com/scala/)

Exercises

- [99 Scala Problems](http://aperiodic.net/phil/scala/s-99/)
- [Programming](https://github.com/karan/Projects) [Projects](http://www.dreamincode.net/forums/topic/78802-martyr2s-mega-project-ideas-list/)

---

# Understanding Python Decorators

from [stackoverflow.com](http://stackoverflow.com/questions/739654/understanding-python-decorators#1594484) . None of the text below is my own writing:

---

Since [this answer](http://stackoverflow.com/questions/231767/can-somebody-explain-me-the-python-yield-statement/231855#231855) explaining yield has been quite a success, I think a little tutorial about Python decorators could help as well.

If you are not into long explanations, see [Paolo Bergantino's answer](http://stackoverflow.com/questions/739654/understanding-python-decorators#answer-739665).

## Python's functions are objects

To understand decorators, you must first understand that functions are objects in Python. This has important consequences. Let's see why with a simple example :

    def shout(word="yes"):
    	return word.capitalize()+"!"

    print shout()
    # outputs : 'Yes!'

    # As an object, you can assign the function to a variable like any
    # other object

    scream = shout

    # Notice we don't use parentheses: we are not calling the function, we are
    # putting the function "shout" into the variable "scream".
    # It means you can then call "shout" from "scream":

    print scream()
    # outputs : 'Yes!'

    # More than that, it means you can remove the old name 'shout', and
    # the function will still be accessible from 'scream'

    del shout
    try:
    	print shout()
    except NameError, e:
    	print e
    	#outputs: "name 'shout' is not defined"

    print scream()
    # outputs: 'Yes!'

OK, keep that in mind, we are going back to it soon. Another interesting property of Python functions is they can be defined... inside another function!

    def talk():

    	# You can define a function on the fly in "talk" ...
    	def whisper(word="yes"):
    		return word.lower()+"..."

    	# ... and use it right away!

    	print whisper()

    # You call "talk", that defines "whisper" EVERY TIME you call it, then
    # "whisper" is called in "talk".
    talk()
    # outputs:
    # "yes..."

    # But "whisper" DOES NOT EXIST outside "talk":

    try:
    	print whisper()
    except NameError, e:
    	print e
    	#outputs : "name 'whisper' is not defined"*

## Functions references

OK, still here? Now the fun part, you've seen that functions are objects and therefore:

- can be assigned to a variable;
- can be defined in another function.

Well, that means that a function can return another function :-) Have a look:

    def getTalk(type="shout"):

    	# We define functions on the fly
    	def shout(word="yes"):
    		return word.capitalize()+"!"

    	def whisper(word="yes") :
    		return word.lower()+"...";

    	# Then we return one of them
    	if type == "shout":
    		# We don't use "()", we are not calling the function,
    		# we are returning the function object
    		return shout
    	else:
    		return whisper

    # How do you use this strange beast?

    # Get the function and assign it to a variable
    talk = getTalk()

    # You can see that "talk" is here a function object:
    print talk
    #outputs : <function shout at 0xb7ea817c>

    # The object is the one returned by the function:
    print talk()
    #outputs : Yes!

    # And you can even use it directly if you feel wild:
    print getTalk("whisper")()
    #outputs : yes...

But wait, there is more. If you can return a function, then you can pass one as a parameter:

    def doSomethingBefore(func):
    	print "I do something before then I call the function you gave me"
    	print func()

    doSomethingBefore(scream)
    #outputs:
    #I do something before then I call the function you gave me
    #Yes!

Well, you just have everything needed to understand decorators. You see, decorators are wrappers which means that **they let you execute code before and after the function they decorate** without the need to modify the function itself.

## Handcrafted decorators

How you would do it manually:

    # A decorator is a function that expects ANOTHER function as parameter
    def my_shiny_new_decorator(a_function_to_decorate):

    	# Inside, the decorator defines a function on the fly: the wrapper.
    	# This function is going to be wrapped around the original function
    	# so it can execute code before and after it.
    	def the_wrapper_around_the_original_function():

    		# Put here the code you want to be executed BEFORE the original
    		# function is called
    		print "Before the function runs"

    		# Call the function here (using parentheses)
    		a_function_to_decorate()

    		# Put here the code you want to be executed AFTER the original
    		# function is called
    		print "After the function runs"

    	# At this point, "a_function_to_decorate" HAS NEVER BEEN EXECUTED.
    	# We return the wrapper function we have just created.
    	# The wrapper contains the function and the code to execute before
    	# and after. It's ready to use!
    	return the_wrapper_around_the_original_function

    # Now imagine you create a function you don't want to ever touch again.
    def a_stand_alone_function():
    	print "I am a stand alone function, don't you dare modify me"

    a_stand_alone_function()
    #outputs: I am a stand alone function, don't you dare modify me

    # Well, you can decorate it to extend its behavior.
    # Just pass it to the decorator, it will wrap it dynamically in
    # any code you want and return you a new function ready to be used:

    a_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)
    a_stand_alone_function_decorated()
    #outputs:
    #Before the function runs
    #I am a stand alone function, don't you dare modify me
    #After the function runs

Now, you probably want that every time you call `a_stand_alone_function`, `a_stand_alone_function_decorated` is called instead. That's easy, just overwrite `a_stand_alone_function` with the function returned by `my_shiny_new_decorator`:

    a_stand_alone_function = my_shiny_new_decorator(a_stand_alone_function)
    a_stand_alone_function()
    #outputs:
    #Before the function runs
    #I am a stand alone function, don't you dare modify me
    #After the function runs

    # And guess what? That's EXACTLY what decorators do!

## Decorators demystified

The previous example, using the decorator syntax:

    @my_shiny_new_decorator
    def another_stand_alone_function():
    	print "Leave me alone"

    another_stand_alone_function()
    #outputs:
    #Before the function runs
    #Leave me alone
    #After the function runs

Yes, that's all, it's that simple. `@decorator` is just a shortcut to:

    another_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)

Decorators are just a pythonic variant of the [decorator design pattern](http://en.wikipedia.org/wiki/Decorator_pattern). There are several classic design patterns embedded in Python to ease development, like iterators.

Of course, you can cumulate decorators:

    def bread(func):
    	def wrapper():
    		print "</''''''\>"
    		func()
    		print "<\______/>"
    	return wrapper

    def ingredients(func):
    	def wrapper():
    		print "#tomatoes#"
    		func()
    		print "~salad~"
    	return wrapper

    def sandwich(food="--ham--"):
    	print food

    sandwich()
    #outputs: --ham--
    sandwich = bread(ingredients(sandwich))
    sandwich()
    #outputs:
    #</''''''\>
    # #tomatoes#
    # --ham--
    # ~salad~
    #<\______/>

Using the Python decorator syntax:

    @bread
    @ingredients
    def sandwich(food="--ham--"):
    	print food

    sandwich()
    #outputs:
    #</''''''\>
    # #tomatoes#
    # --ham--
    # ~salad~
    #<\______/>

The order you set the decorators MATTERS:

    @ingredients
    @bread
    def strange_sandwich(food="--ham--"):
    	print food

    strange_sandwich()
    #outputs:
    ##tomatoes#
    #</''''''\>
    # --ham--
    #<\______/>
    # ~salad~

## Eventually answering the question

As a conclusion, you can easily see how to answer the question:

    # The decorator to make it bold
    def makebold(fn):
    	# The new function the decorator returns
    	def wrapper():
    		# Insertion of some code before and after
    		return "<b>" + fn() + "</b>"
    	return wrapper

    # The decorator to make it italic
    def makeitalic(fn):
    	# The new function the decorator returns
    	def wrapper():
    		# Insertion of some code before and after
    		return "<i>" + fn() + "</i>"
    	return wrapper

    @makebold
    @makeitalic
    def say():
    	return "hello"

    print say()
    #outputs: <b><i>hello</i></b>

    # This is the exact equivalent to
    def say():
    	return "hello"
    say = makebold(makeitalic(say))

    print say()
    #outputs: <b><i>hello</i></b>

You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.

## Passing arguments to the decorated function

    # It's not black magic, you just have to let the wrapper
    # pass the argument:

    def a_decorator_passing_arguments(function_to_decorate):
    	def a_wrapper_accepting_arguments(arg1, arg2):
    		print "I got args! Look:", arg1, arg2
    		function_to_decorate(arg1, arg2)
    	return a_wrapper_accepting_arguments

    # Since when you are calling the function returned by the decorator, you are
    # calling the wrapper, passing arguments to the wrapper will let it pass them to
    # the decorated function

    @a_decorator_passing_arguments
    def print_full_name(first_name, last_name):
    	print "My name is", first_name, last_name

    print_full_name("Peter", "Venkman")
    # outputs:
    #I got args! Look: Peter Venkman
    #My name is Peter Venkman

## Decorating methods

What's great with Python is that methods and functions are really the same, except methods expect their first parameter to be a reference to the current object (`self`). It means you can build a decorator for methods the same way, just remember to take self in consideration:

    def method_friendly_decorator(method_to_decorate):
    	def wrapper(self, lie):
    		lie = lie - 3 # very friendly, decrease age even more :-)
    		return method_to_decorate(self, lie)
    	return wrapper


    class Lucy(object):

    	def __init__(self):
    		self.age = 32

    	@method_friendly_decorator
    	def sayYourAge(self, lie):
    		print "I am %s, what did you think?" % (self.age + lie)

    l = Lucy()
    l.sayYourAge(-3)
    #outputs: I am 26, what did you think?

Of course, if you make a very general decorator and want to apply it to any function or method, no matter its arguments, then just use `*args, **kwargs`:

    def a_decorator_passing_arbitrary_arguments(function_to_decorate):
    	# The wrapper accepts any arguments
    	def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):
    		print "Do I have args?:"
    		print args
    		print kwargs
    		# Then you unpack the arguments, here *args, **kwargs
    		# If you are not familiar with unpacking, check:
    		# http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/
    		function_to_decorate(*args, **kwargs)
    	return a_wrapper_accepting_arbitrary_arguments

    @a_decorator_passing_arbitrary_arguments
    def function_with_no_argument():
    	print "Python is cool, no argument here."

    function_with_no_argument()
    #outputs
    #Do I have args?:
    #()
    #{}
    #Python is cool, no argument here.

    @a_decorator_passing_arbitrary_arguments
    def function_with_arguments(a, b, c):
    	print a, b, c

    function_with_arguments(1,2,3)
    #outputs
    #Do I have args?:
    #(1, 2, 3)
    #{}
    #1 2 3

    @a_decorator_passing_arbitrary_arguments
    def function_with_named_arguments(a, b, c, platypus="Why not ?"):
    	print "Do %s, %s and %s like platypus? %s" %\
    	(a, b, c, platypus)

    function_with_named_arguments("Bill", "Linus", "Steve", platypus="Indeed!")
    #outputs
    #Do I have args ? :
    #('Bill', 'Linus', 'Steve')
    #{'platypus': 'Indeed!'}
    #Do Bill, Linus and Steve like platypus? Indeed!

    class Mary(object):

    	def __init__(self):
    		self.age = 31

    	@a_decorator_passing_arbitrary_arguments
    	def sayYourAge(self, lie=-3): # You can now add a default value
    		print "I am %s, what did you think ?" % (self.age + lie)

    m = Mary()
    m.sayYourAge()
    #outputs
    # Do I have args?:
    #(<__main__.Mary object at 0xb7d303ac>,)
    #{}
    #I am 28, what did you think?

## Passing arguments to the decorator

Great, now what would you say about passing arguments to the decorator itself? Well this is a bit twisted because a decorator must accept a function as an argument and therefore, you cannot pass the decorated function arguments directly to the decorator.

Before rushing to the solution, let's write a little reminder:

    # Decorators are ORDINARY functions
    def my_decorator(func):
    	print "I am a ordinary function"
    	def wrapper():
    		print "I am function returned by the decorator"
    		func()
    	return wrapper

    # Therefore, you can call it without any "@"

    def lazy_function():
    	print "zzzzzzzz"

    decorated_function = my_decorator(lazy_function)
    #outputs: I am a ordinary function

    # It outputs "I am a ordinary function", because that's just what you do:
    # calling a function. Nothing magic.

    @my_decorator
    def lazy_function():
    	print "zzzzzzzz"

    #outputs: I am a ordinary function

It's exactly the same. "`my_decorator`" is called. So when you `@my_decorator`, you are telling Python to call the function 'labeled by the variable "`my_decorator`"'. It's important, because the label you give can point directly to the decorator... or not! Let's start to be evil!

    def decorator_maker():

    	print "I make decorators! I am executed only once: "+\
    		  "when you make me create a decorator."

    	def my_decorator(func):

    		print "I am a decorator! I am executed only when you decorate a function."

    		def wrapped():
    			print ("I am the wrapper around the decorated function. "
    				  "I am called when you call the decorated function. "
    				  "As the wrapper, I return the RESULT of the decorated function.")
    			return func()

    		print "As the decorator, I return the wrapped function."

    		return wrapped

    	print "As a decorator maker, I return a decorator"
    	return my_decorator

    # Let's create a decorator. It's just a new function after all.
    new_decorator = decorator_maker()
    #outputs:
    #I make decorators! I am executed only once: when you make me create a decorator.
    #As a decorator maker, I return a decorator

    # Then we decorate the function

    def decorated_function():
    	print "I am the decorated function."

    decorated_function = new_decorator(decorated_function)
    #outputs:
    #I am a decorator! I am executed only when you decorate a function.
    #As the decorator, I return the wrapped function

    # Let's call the function:
    decorated_function()
    #outputs:
    #I am the wrapper around the decorated function. I am called when you call the decorated function.
    #As the wrapper, I return the RESULT of the decorated function.
    #I am the decorated function.

No surprise here. Let's do EXACTLY the same thing, but skipping intermediate variables:

    def decorated_function():
    	print "I am the decorated function."
    decorated_function = decorator_maker()(decorated_function)
    #outputs:
    #I make decorators! I am executed only once: when you make me create a decorator.
    #As a decorator maker, I return a decorator
    #I am a decorator! I am executed only when you decorate a function.
    #As the decorator, I return the wrapped function.

    # Finally:
    decorated_function()
    #outputs:
    #I am the wrapper around the decorated function. I am called when you call the decorated function.
    #As the wrapper, I return the RESULT of the decorated function.
    #I am the decorated function.

Let's make it AGAIN, even shorter:

    @decorator_maker()
    def decorated_function():
    	print "I am the decorated function."
    #outputs:
    #I make decorators! I am executed only once: when you make me create a decorator.
    #As a decorator maker, I return a decorator
    #I am a decorator! I am executed only when you decorate a function.
    #As the decorator, I return the wrapped function.

    #Eventually:
    decorated_function()
    #outputs:
    #I am the wrapper around the decorated function. I am called when you call the decorated function.
    #As the wrapper, I return the RESULT of the decorated function.
    #I am the decorated function.

Hey, did you see that? We used a function call with the "@" syntax :-)

So back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?

    def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):

    	print "I make decorators! And I accept arguments:", decorator_arg1, decorator_arg2

    	def my_decorator(func):
    		# The ability to pass arguments here is a gift from closures.
    		# If you are not comfortable with closures, you can assume it's ok,
    		# or read: http://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python
    		print "I am the decorator. Somehow you passed me arguments:", decorator_arg1, decorator_arg2

    		# Don't confuse decorator arguments and function arguments!
    		def wrapped(function_arg1, function_arg2) :
    			print ("I am the wrapper around the decorated function.\n"
    				  "I can access all the variables\n"
    				  "\t- from the decorator: {0} {1}\n"
    				  "\t- from the function call: {2} {3}\n"
    				  "Then I can pass them to the decorated function"
    				  .format(decorator_arg1, decorator_arg2,
    						  function_arg1, function_arg2))
    			return func(function_arg1, function_arg2)

    		return wrapped

    	return my_decorator

    @decorator_maker_with_arguments("Leonard", "Sheldon")
    def decorated_function_with_arguments(function_arg1, function_arg2):
    	print ("I am the decorated function and only knows about my arguments: {0}"
    		   " {1}".format(function_arg1, function_arg2))

    decorated_function_with_arguments("Rajesh", "Howard")
    #outputs:
    #I make decorators! And I accept arguments: Leonard Sheldon
    #I am the decorator. Somehow you passed me arguments: Leonard Sheldon
    #I am the wrapper around the decorated function.
    #I can access all the variables
    #   - from the decorator: Leonard Sheldon
    #   - from the function call: Rajesh Howard
    #Then I can pass them to the decorated function
    #I am the decorated function and only knows about my arguments: Rajesh Howard

Here it is, a decorator with arguments. Arguments can be set as variable:

    c1 = "Penny"
    c2 = "Leslie"

    @decorator_maker_with_arguments("Leonard", c1)
    def decorated_function_with_arguments(function_arg1, function_arg2):
    	print ("I am the decorated function and only knows about my arguments:"
    		   " {0} {1}".format(function_arg1, function_arg2))

    decorated_function_with_arguments(c2, "Howard")
    #outputs:
    #I make decorators! And I accept arguments: Leonard Penny
    #I am the decorator. Somehow you passed me arguments: Leonard Penny
    #I am the wrapper around the decorated function.
    #I can access all the variables
    #   - from the decorator: Leonard Penny
    #   - from the function call: Leslie Howard
    #Then I can pass them to the decorated function
    #I am the decorated function and only knows about my arguments: Leslie Howard

As you can see, you can pass arguments to the decorator like any function using this trick. You can even use \*args, \*\*kwargs if you wish. But remember decorators are called only once. Just when Python imports the script. You can't dynamically set the arguments afterwards. When you do "import x", the function is already decorated, so you can't change anything.

## Let's practice: a decorator to decorate a decorator

OK, as a bonus, I'll give you a snippet to make any decorator accept generically any argument. After all, in order to accept arguments, we created our decorator using another function. We wrapped the decorator. Anything else we saw recently that wrapped function? Oh yes, decorators! Let's have some fun and write a decorator for the decorators:

    def decorator_with_args(decorator_to_enhance):
    	"""
    	This function is supposed to be used as a decorator.
    	It must decorate an other function, that is intended to be used as a decorator.
    	Take a cup of coffee.
    	It will allow any decorator to accept an arbitrary number of arguments,
    	saving you the headache to remember how to do that every time.
    	"""

    	# We use the same trick we did to pass arguments
    	def decorator_maker(*args, **kwargs):

    		# We create on the fly a decorator that accepts only a function
    		# but keeps the passed arguments from the maker.
    		def decorator_wrapper(func):

    			# We return the result of the original decorator, which, after all,
    			# IS JUST AN ORDINARY FUNCTION (which returns a function).
    			# Only pitfall: the decorator must have this specific signature or it won't work:
    			return decorator_to_enhance(func, *args, **kwargs)

    		return decorator_wrapper

    	return decorator_maker

It can be used as follows:

    # You create the function you will use as a decorator. And stick a decorator on it :-)
    # Don't forget, the signature is "decorator(func, *args, **kwargs)"
    @decorator_with_args
    def decorated_decorator(func, *args, **kwargs):
    	def wrapper(function_arg1, function_arg2):
    		print "Decorated with", args, kwargs
    		return func(function_arg1, function_arg2)
    	return wrapper

    # Then you decorate the functions you wish with your brand new decorated decorator.

    @decorated_decorator(42, 404, 1024)
    def decorated_function(function_arg1, function_arg2):
    	print "Hello", function_arg1, function_arg2

    decorated_function("Universe and", "everything")
    #outputs:
    #Decorated with (42, 404, 1024) {}
    #Hello Universe and everything

    # Whoooot!

I know, the last time you had this feeling, it was after listening a guy saying: "before understanding recursion, you must first understand recursion". But now, don't you feel good about mastering this?

## Best practices while using decorators

- They are new as of Python 2.4, so be sure that's what your code is running on.
- Decorators slow down the function call. Keep that in mind.
- You can not un-decorate a function. There are hacks to create decorators that can be removed but nobody uses them. So once a function is decorated, it's done. For all the code.
- Decorators wrap functions, which can make them hard to debug.

Python 2.5 solves this last issue by providing the `functools` module including `functools.wraps` that copies the name, module and docstring of any wrapped function to it's wrapper. Fun fact, `functools.wraps ` is a decorator :-)

    # For debugging, the stacktrace prints you the function __name__
    def foo():
    	print "foo"

    print foo.__name__
    #outputs: foo

    # With a decorator, it gets messy
    def bar(func):
    	def wrapper():
    		print "bar"
    		return func()
    	return wrapper

    @bar
    def foo():
    	print "foo"

    print foo.__name__
    #outputs: wrapper

    # "functools" can help for that

    import functools

    def bar(func):
    	# We say that "wrapper", is wrapping "func"
    	# and the magic begins
    	@functools.wraps(func)
    	def wrapper():
    		print "bar"
    		return func()
    	return wrapper

    @bar
    def foo():
    	print "foo"

    print foo.__name__
    #outputs: foo

## How can the decorators be useful?

Now the big question: what can I use decorators for? Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities. Classic uses are extending a function behavior from an external lib (you can't modify it) or for a debug purpose (you don't want to modify it because it's temporary). You can use them to extends several functions with the same code without rewriting it every time, for DRY's sake. E.g.:

    def benchmark(func):
    	"""
    	A decorator that prints the time a function takes
    	to execute.
    	"""
    	import time
    	def wrapper(*args, **kwargs):
    		t = time.clock()
    		res = func(*args, **kwargs)
    		print func.__name__, time.clock()-t
    		return res
    	return wrapper


    def logging(func):
    	"""
    	A decorator that logs the activity of the script.
    	(it actually just prints it, but it could be logging!)
    	"""
    	def wrapper(*args, **kwargs):
    		res = func(*args, **kwargs)
    		print func.__name__, args, kwargs
    		return res
    	return wrapper


    def counter(func):
    	"""
    	A decorator that counts and prints the number of times a function has been executed
    	"""
    	def wrapper(*args, **kwargs):
    		wrapper.count = wrapper.count + 1
    		res = func(*args, **kwargs)
    		print "{0} has been used: {1}x".format(func.__name__, wrapper.count)
    		return res
    	wrapper.count = 0
    	return wrapper

    @counter
    @benchmark
    @logging
    def reverse_string(string):
    	return str(reversed(string))

    print reverse_string("Able was I ere I saw Elba")
    print reverse_string("A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!")

    #outputs:
    #reverse_string ('Able was I ere I saw Elba',) {}
    #wrapper 0.0
    #wrapper has been used: 1x
    #ablE was I ere I saw elbA
    #reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}
    #wrapper 0.0
    #wrapper has been used: 2x
    #!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A

Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:

    @counter
    @benchmark
    @logging
    def get_random_futurama_quote():
    	import httplib
    	conn = httplib.HTTPConnection("slashdot.org:80")
    	conn.request("HEAD", "/index.html")
    	for key, value in conn.getresponse().getheaders():
    		if key.startswith("x-b") or key.startswith("x-f"):
    			return value
    	return "No, I'm ... doesn't!"

    print get_random_futurama_quote()
    print get_random_futurama_quote()

    #outputs:
    #get_random_futurama_quote () {}
    #wrapper 0.02
    #wrapper has been used: 1x
    #The laws of science be a harsh mistress.
    #get_random_futurama_quote () {}
    #wrapper 0.01
    #wrapper has been used: 2x
    #Curse you, merciful Poseidon!

Python itself provides several decorators: `property`, `staticmethod`, etc. Django use decorators to manage caching and view permissions. Twisted to fake inlining asynchronous functions calls. This really is a large playground.

EDIT: given the success of this answer, and people asking me to do the same with metaclasses, [I did](http://stackoverflow.com/questions/100003/what-is-a-metaclass-in-python/6581949#6581949).

---

# The State of Classical Music

from [reddit.com](http://www.reddit.com/r/composertalk/comments/1b9dwi/the_condition_of_classical_music_or_serious_music/c95byag) user [yajnavalkya](http://www.reddit.com/user/yajnavalkya). None of the text below is my own writing:

---

As a young avant garde composer this is a topic in which I am very interested and I think it boils down to one real conflict. People who are "serious" about music have different desires than people who are casual music appreciators.

The casual musician generally seems to want music to reflect emotional states that are parallel to their own. When somebody listens to a beautiful piece of music they hear it in terms of how it "feels" to them. There was a time in classical music where composers composed that way too. Every composer wanted to capture the depths of the human experience, and the most successful composers were specifically writing about their own painful love affairs and such. The romantic era, because of this, is basically the ticket seller of classical music.

It actually is quite interesting, then, to compare the goals of various musicians. Before Beethoven, most composers were writing their music not to be listened to, but to be played. Playing music was considered a fun past time for the wealthy and only rarely would it be done for the sake of an audience. This performer centric view of music is why Mozart and Bach are generally considered to be musical geniuses, but are put on classical programs less often than Beethoven through Mahler. Especially in regards to their chamber music: their compositions weren't primarily about being listened to, they were about being played. Interestingly, there is a whole lost art in the performance of that music, but that's a discussion for another day.

Since the romantic era the goal of "serious" composers has changed yet again. If you spend a lot of time working with music, before long the idioms such as: major keys for joy; minor keys for sorrow; the cadence; solo violin for moments of emotional tension; fast 6/8 motives to represent water; etc; start to seem completely arbitrary. Why does any of that have to be? If I'm happy and I sit at the piano I could just as easily express my happiness by banging giant twelve tone clusters as playing a few chords in C major. And if you accept that happiness isn't innately related to Major chords, and could be just as well expressed by clusters, before long, even the idea of emotion in music starts to seem really petty. In my opinion, expressing emotion is actually not something that music is particularly good at, rather having emotions is something that audiences are really good at.

I stand by Stravinsky when he wrote:

> For I consider that music is, by its very nature, essentially powerless to express anything at all, whether a feeling, an attitude of mind, a psychological mood, a phenomenon of nature, etc. Expression has never been an inherent property of music. That is by no means the purpose of its existence. If, as is nearly always the case, music appears to express something, this is only an illusion and not a reality. It is simply an additional attribute which, by tacit and inveterate agreement, we have lent it, thrust upon it, as a label, a convention - in short, an aspect which, unconsciously or by force of habit, we have come to confuse with its essential being."

The modern attitude becomes then: why use something as powerful as music for something as silly as a broken heart? Especially when it's such a trodden path. If you are claiming that your experience of a broken heart is unique and valuable and interesting, then you have to make it sound unique and valuable and interesting. But even then, I'd submit that the goal of using music to express something like that isn't worth your time, because music is capable of so much more.

Milton Babbitt wrote an infamous essay entitled "The Composer as a Specialist," which was later retitled without his permission "Who Cares if you Listen?" This essay basically laid down the case for composers not as producers of popular music, but as an isolated community of scientists. The basic argument was that music is complex and interesting and composers are studying the very nature of music by composing, not producing music for consumption. If the general public is not expected to understand the workings of cutting edge quantum physics, why should they be expected to understand and appreciate the cutting edge of music?

And whether or not you agree with that idea as a goal, that's what happened. Music, due to it's astounding power and seemingly inexhaustible complexity, outgrew the idea of petty emotion. The composers who are on the cutting edge no longer have their own humanity in common with the average person, so the average person feels no compulsion to listen to them. Quite specifically because there is nothing there for them.

If you don't understand Milton Babbitt's music that's ok, because it wasn't for you in the first place. This is more nuanced than just saying Babbitt was an elitist, though he's been maligned that way since the essay was published. Instead Babbitt just recognized that music can be complex and if somebody wants to appreciate it they're going to have to learn a little bit first.

Finally, pop music is an interesting subject. There was always experimental pop music. We take it for granted nowadays, but the late 50s through early 70s were such a revolutionary time for pop music. And these moments of revolution show us that people want more than just the same thing over and over again. Naturally the Beetles got big on their love ballades, but it was the way that their love ballades were written that was so shocking to the older generation at the time. Furthermore, the anti-war music that was produced showed that people were ready for music to make a statement too, not just be a casual past time.

Today, the pop music industry has gotten more cynical. The statements have all been bought, and the industry has gotten amazingly good at marketing music as a product. And for the casual listener that is preferable, because pop music today is about getting the most dopamine for the least amount of listening effort. If you consider how easy it is to listen to 2-5 minutes of pop music and feel the same joy or heartbreak that Wagner would pull out of you over 4 hours then you can see why classical music frankly can't compete in today's world. And it's very hard to explain to a skeptic why 4 hours of Wagner is valuable if they can get the same emotional experience in 2 minutes. Unfortunately the nature of capitalism pits high art music in direct competition with the pop music industry and so they both can't really cohabitate completely. Which is too bad, because they both certainly deserve a place on this planet.

> tl;dr Modern concert music isn't for the average listener. Romantic music has the same goal as the average listener, but doesn't give nearly as much reward for the cost of listening as pop music. Classical music in general has too many different goals for it to have broad appeal. Classical music needs to be protected from competition better, but that certainly won't happen in America where it's hard to get people enough government support let alone art.

---

# The Fallacy of Success

> reproduced under terms of use [http://www.gutenberg.org/](http://www.gutenberg.org/). For original link see [here](http://www.gutenberg.org/files/11505/11505-h/11505-h.htm#THE_FALLACY_OF_SUCCESS).

There has appeared in our time a particular class of books and articles which I sincerely and solemnly think may be called the silliest ever known among men. They are much more wild than the wildest romances of chivalry and much more dull than the dullest religious tract. Moreover, the romances of chivalry were at least about chivalry; the religious tracts are about religion. But these things are about nothing; they are about what is called Success. On every bookstall, in every magazine, you may find works telling people how to succeed. They are books showing men how to succeed in everything; they are written by men who cannot even succeed in writing books. To begin with, of course, there is no such thing as Success. Or, if you like to put it so, there is nothing that is not successful. That a thing is successful merely means that it is; a millionaire is successful in being a millionaire and a donkey in being a donkey. Any live man has succeeded in living; any dead man may have succeeded in committing suicide. But, passing over the bad logic and bad philosophy in the phrase, we may take it, as these writers do, in the ordinary sense of success in obtaining money or worldly position. These writers profess to tell the ordinary man how he may succeed in his trade or speculation-how, if he is a builder, he may succeed as a builder; how, if he is a stockbroker, he may succeed as a stockbroker. They profess to show him how, if he is a grocer, he may become a sporting yachtsman; how, if he is a tenth-rate journalist, he may become a peer; and how, if he is a German Jew, he may become an Anglo-Saxon. This is a definite and business-like proposal, and I really think that the people who buy these books (if any people do buy them) have a moral, if not a legal, right to ask for their money back. Nobody would dare to publish a book about electricity which literally told one nothing about electricity; no one would dare to publish an article on botany which showed that the writer did not know which end of a plant grew in the earth. Yet our modern world is full of books about Success and successful people which literally contain no kind of idea, and scarcely any kind of verbal sense.

It is perfectly obvious that in any decent occupation (such as bricklaying or writing books) there are only two ways (in any special sense) of succeeding. One is by doing very good work, the other is by cheating. Both are much too simple to require any literary explanation. If you are in for the high jump, either jump higher than any one else, or manage somehow to pretend that you have done so. If you want to succeed at whist, either be a good whist-player, or play with marked cards. You may want a book about jumping; you may want a book about whist; you may want a book about cheating at whist. But you cannot want a book about Success. Especially you cannot want a book about Success such as those which you can now find scattered by the hundred about the book-market. You may want to jump or to play cards; but you do not want to read wandering statements to the effect that jumping is jumping, or that games are won by winners. If these writers, for instance, said anything about success in jumping it would be something like this: "The jumper must have a clear aim before him. He must desire definitely to jump higher than the other men who are in for the same competition. He must let no feeble feelings of mercy (sneaked from the sickening Little Englanders and Pro-Boers) prevent him from trying to do his best. He must remember that a competition in jumping is distinctly competitive, and that, as Darwin has gloriously demonstrated, THE WEAKEST GO TO THE WALL." That is the kind of thing the book would say, and very useful it would be, no doubt, if read out in a low and tense voice to a young man just about to take the high jump. Or suppose that in the course of his intellectual rambles the philosopher of Success dropped upon our other case, that of playing cards, his bracing advice --"In playing cards it is very necessary to avoid the mistake (commonly made by maudlin humanitarians and Free Traders) of permitting your opponent to win the game. You must have grit and snap and go in to win. The days of idealism and superstition are over. We live in a time of science and hard common sense, and it has now been definitely proved that in any game where two are playing IF ONE DOES NOT WIN THE OTHER WILL." It is all very stirring, of course; but I confess that if I were playing cards I would rather have some decent little book which told me the rules of the game. Beyond the rules of the game it is all a question either of talent or dishonesty; and I will undertake to provide either one or the other-which, it is not for me to say.

Turning over a popular magazine, I find a queer and amusing example. There is an article called "The Instinct that Makes People Rich." It is decorated in front with a formidable portrait of Lord Rothschild. There are many definite methods, honest and dishonest, which make people rich; the only "instinct" I know of which does it is that instinct which theological Christianity crudely describes as "the sin of avarice." That, however, is beside the present point. I wish to quote the following exquisite paragraphs as a piece of typical advice as to how to succeed. It is so practical; it leaves so little doubt about what should be our next step-

"The name of Vanderbilt is synonymous with wealth gained by modern enterprise. 'Cornelius,' the founder of the family, was the first of the great American magnates of commerce. He started as the son of a poor farmer; he ended as a millionaire twenty times over.

"He had the money-making instinct. He seized his opportunities, the opportunities that were given by the application of the steam-engine to ocean traffic, and by the birth of railway locomotion in the wealthy but undeveloped United States of America, and consequently he amassed an immense fortune.

"Now it is, of course, obvious that we cannot all follow exactly in the footsteps of this great railway monarch. The precise opportunities that fell to him do not occur to us. Circumstances have changed. But, although this is so, still, in our own sphere and in our own circumstances, we can follow his general methods; we can seize those opportunities that are given us, and give ourselves a very fair chance of attaining riches."

In such strange utterances we see quite clearly what is really at the bottom of all these articles and books. It is not mere business; it is not even mere cynicism. It is mysticism; the horrible mysticism of money. The writer of that passage did not really have the remotest notion of how Vanderbilt made his money, or of how anybody else is to make his. He does, indeed, conclude his remarks by advocating some scheme; but it has nothing in the world to do with Vanderbilt. He merely wished to prostrate himself before the mystery of a millionaire. For when we really worship anything, we love not only its clearness but its obscurity. We exult in its very invisibility. Thus, for instance, when a man is in love with a woman he takes special pleasure in the fact that a woman is unreasonable. Thus, again, the very pious poet, celebrating his Creator, takes pleasure in saying that God moves in a mysterious way. Now, the writer of the paragraph which I have quoted does not seem to have had anything to do with a god, and I should not think (judging by his extreme unpracticality) that he had ever been really in love with a woman. But the thing he does worship-Vanderbilt-he treats in exactly this mystical manner. He really revels in the fact his deity Vanderbilt is keeping a secret from him. And it fills his soul with a sort of transport of cunning, an ecstasy of priestcraft, that he should pretend to be telling to the multitude that terrible secret which he does not know.

Speaking about the instinct that makes people rich, the same writer remarks---

"In olden days its existence was fully understood. The Greeks enshrined it in the story of Midas, of the 'Golden Touch.' Here was a man who turned everything he laid his hands upon into gold. His life was a progress amidst riches. Out of everything that came in his way he created the precious metal. 'A foolish legend,' said the wiseacres of the Victorian age. 'A truth,' say we of to-day. We all know of such men. We are ever meeting or reading about such persons who turn everything they touch into gold. Success dogs their very footsteps. Their life's pathway leads unerringly upwards. They cannot fail."

Unfortunately, however, Midas could fail; he did. His path did not lead unerringly upward. He starved because whenever he touched a biscuit or a ham sandwich it turned to gold. That was the whole point of the story, though the writer has to suppress it delicately, writing so near to a portrait of Lord Rothschild. The old fables of mankind are, indeed, unfathomably wise; but we must not have them expurgated in the interests of Mr. Vanderbilt. We must not have King Midas represented as an example of success; he was a failure of an unusually painful kind. Also, he had the ears of an ass. Also (like most other prominent and wealthy persons) he endeavoured to conceal the fact. It was his barber (if I remember right) who had to be treated on a confidential footing with regard to this peculiarity; and his barber, instead of behaving like a go-ahead person of the Succeed-at-all-costs school and trying to blackmail King Midas, went away and whispered this splendid piece of society scandal to the reeds, who enjoyed it enormously. It is said that they also whispered it as the winds swayed them to and fro. I look reverently at the portrait of Lord Rothschild; I read reverently about the exploits of Mr. Vanderbilt. I know that I cannot turn everything I touch to gold; but then I also know that I have never tried, having a preference for other substances, such as grass, and good wine. I know that these people have certainly succeeded in something; that they have certainly overcome somebody; I know that they are kings in a sense that no men were ever kings before; that they create markets and bestride continents. Yet it always seems to me that there is some small domestic fact that they are hiding, and I have sometimes thought I heard upon the wind the laughter and whisper of the reeds.

At least, let us hope that we shall all live to see these absurd books about Success covered with a proper derision and neglect. They do not teach people to be successful, but they do teach people to be snobbish; they do spread a sort of evil poetry of worldliness. The Puritans are always denouncing books that inflame lust; what shall we say of books that inflame the viler passions of avarice and pride? A hundred years ago we had the ideal of the Industrious Apprentice; boys were told that by thrift and work they would all become Lord Mayors. This was fallacious, but it was manly, and had a minimum of moral truth. In our society, temperance will not help a poor man to enrich himself, but it may help him to respect himself. Good work will not make him a rich man, but good work may make him a good workman. The Industrious Apprentice rose by virtues few and narrow indeed, but still virtues. But what shall we say of the gospel preached to the new Industrious Apprentice; the Apprentice who rises not by his virtues, but avowedly by his vices?

---

# Goodbye Microsoft Hello Facebook

I found this farewell letter to be full of advice which probably is quite relevant to me.

> Use Occam's Razor in interpersonal relations: look for the simplest, most straightforward explanation that assumes the best of everybody. Stay away from people who always have a conspiracy theory involving twisted office politics, unfulfilled Machiavellian ambitions, and unspoken agendas.

> > Good people with good process will outperform good people with no process every time. - Grady Booch

> Don't fear process. Fear bad people dictating process. Fear process trying to make up for bad people.  
>  I've managed almost 150 people across dev/test/PM. I estimate about 60% of employees think that they belong in the top 20% when ranked against their peers. I have never once had a person say that they belong in the bottom 10%.

> Offer me one great Microsoft engineer for five "solid" ones: I gladly take the exchange.

> In a company as large as Microsoft, I guarantee you'll find someone higher level than you who you think is worse than you. Don't get stuck in this mental trap - it won't motivate you to be your best. Look instead towards the person you admire most at your level. What can you learn from them? What unique strengths might you have which they don't have?

[Philip Su](http://worldofsu.com/philipsu/goodbye-microsoft-hello-facebook/)

---

# Teddy Bear Debugging

Teddy bear debugging (or [rubber duck debugging](http://en.wikipedia.org/wiki/Rubber_duck_debugging)) involves telling your problems to a teddy bear!

Although amusing (and cute), why does this actually work?

I think it follows the idea that:

> To teach is to learn twice. -- [Joseph Joubert](http://en.wikipedia.org/wiki/Joseph_Joubert)

It ensures that you have a firm grasp of what you're doing, and perhaps more importantly, forces you to slow down and think.

Though another person's theory to is:

> [DanielDennett](http://www.c2.com/cgi/wiki?DanielDennett) has a good theory for why this helps. He puts forth the view (in ConsciousnessExplained?) that consciousness developed as a way to internalize talking to oneself. Speaking words triggers parts of the brain involved in moving the diaphragm, tongue, lips, vocal cords, etc.. Hearing words triggers parts of the brain connected to the ears. Speaking aloud can be a bad survival strategy, especially when you're thinking about the chief's wife, so we developed consciousness as an internal monologue. It works, but it doesn't exercise as many areas of the brain as speaking and hearing your own words. There's no way to test this theory, but it makes sense to me. See [ThinkingOutLoud](http://www.c2.com/cgi/wiki?ThinkingOutLoud) and [SelfTalk](http://www.c2.com/cgi/wiki?SelfTalk) for related explorations.

Here are some other [stories](http://www.c2.com/cgi/wiki?RubberDucking) from [c2.com](http://www.c2.com/cgi/wiki?CardboardProgrammer).

> Basically, there was a university computer programming lab [MIT?] where in one corner sat a perfectly ordinary teddy bear. If you were a student working in this lab, and you were stuck on something, the rule was that you had to explain your problem to the teddy bear first, then go back to your work and try to apply whatever wisdom the bear imparted upon you. If you were still stuck at this point, then and only then were you allowed to ask for the help of the lab's tutor.
>
> If someone comes up to you seeking help with a problem, and during the course of explaining the problem to you they figure it out for themselves, you are being that person's teddy bear. -- cdevers

> At a computer lab at UT (University of Texas) there was a lab assistant who required all people asking for help to first tell their problem to his TeddyBear which lived on his desk. He used to say that after talking to the bear more than 75% of the people solved their problems -- [MarkInterrante](http://www.c2.com/cgi/wiki?MarkInterrante)

---

# No Hitchhikers Guide to Mathematics

>     "Mathematics is useful and I want to be better at it, but I won't write any original proofs and I absolutely abhor reading other people's proofs. I just want to use the theorems others have proved, like Fermat's Last Theorem and the undecidability of the Halting Problem."

The point is that the sentiment is in the wrong place. Mathematics is cousin to programming in terms of the learning curve, obscure culture, and the amount of time one spends confused. And mathematics is as much about writing proofs as software development is about writing programs (it's not everything, but without it you can't do anything). Honestly, it sounds ridiculously obvious to say it directly like this, but the fact remains that people feel like they can understand the content of mathematics without being able to write or read proofs.

[Jeremy Kun](http://jeremykun.com/2013/02/08/why-there-is-no-hitchhikers-guide-to-mathematics-for-programmers/)

---

# Don't Design for Reuse

> If you're designing for reuse, you're doing it wrong.

Perhaps a controversial view (perhaps better titled to be don't reinvent the wheel) but poses the question of (particularly in the comments) that if you write code well, perhaps it will follow anyway.

[Don't Design for Reuse](http://cafe.elharo.com/programming/dont-design-for-reuse/)

---

# The Australian Computer Society should be disbanded

>     Sorry, but who are these f*cking morons? They are doing more damage to the industry than help. I kept thinking that at some point Candid Camera might jump out from behind the door.

[Source](http://www.linkedin.com/today/post/article/20130131000939-921366-the-australian-computer-society-should-be-disbanded)

Lesson: You have to teach yourself. So now I should start setting up schedules and goals...

---

# Farewell to Bioinformatics

> This all seems an inauspicious beginning for a field. Anything so worthless should quickly shrivel up and die, right? Well, intentionally or not, bioinformatics found a way to survive: obfuscation. By making the tools unusable, by inventing file format after file format, by seeking out the most brittle techniques and the slowest languages, by not publishing their algorithms and making their results impossible to replicate, the field managed to reduce its productivity by at least 90%, probably closer to 99%. Thus the thread of failures can be stretched out from years to decades, hidden by the cloak of incompetence...the software is written to be inefficient, to use memory poorly, and the cry goes up for bigger, faster machines! When the machines are procured, even larger hunks of data are indiscriminately shoved through black box implementations of algorithms in hopes that meaning will emerge on the far side. It never does, but maybe with a bigger machine.

Fred Ross [A farewell to bioinformatics](http://madhadron.com/?p=263)

---

###A Loose Technical explanation (Pareto Principle)

Why does the 50/01 follow if 80/20 is true?

Lets assume that the 80/20 is true. This means that the function \\(f(x)\\) has the following properties:

$$f(0) = 0$$
$$f(1) = 1$$
$$f(0.2) = 0.8$$

Since \\(f(x)\\) is a continuous function, we will assume that it follows the power law. That is that \\(f(x) = ax^k\\). By inspection we can conclude that \\(a = 1\\) which remains that we solve \\(k\\). Rearranging equation above yields that \\(k = \frac{ln(0.8)}{ln(0.2)}=\\). Thus

$$f(0.01) = 0.01^{ln(0.8)/ln(0.2)}  \approx 0.5$$

as required.

---

# Seven Programming Languages in Seven Weeks

Bought a new book from the Pragmatic Bookshelf called [Seven Languages in Seven Weeks](http://pragprog.com/book/btlang/seven-languages-in-seven-weeks). I am a big fan of [The Pragmatic Programmer](http://pragprog.com/the-pragmatic-programmer) and I hope this book will be great help to myself.

Learning from the pragmatic programmer: Learn a new language (and use it!)

Alongside this I may also pick up a few database languages to learn as well. Especially looking at all the buzz around NoSQL databases.

First up Ruby.

Things learnt

- Carelessness on my part but the `get` function translates everythings to string type.
- Everything is an object

---

# Probabilistic Inequalities

Probabilistic inequalities are useful to provide quick counter examples in data mining. This is very useful in preventing any issues relating to [data dredging](http://en.wikipedia.org/wiki/Data_dredging).

My current favourite two which are extremely quick and easy to use are:

- Markov inequality (and by corollary, Chebyshev's inequality)
  $$
- Jensen's inequality

---

Four white walls. A single window. One way in, or is it one way out?

Silence.

Well, at least the closest thing you can get to silence; the faint sound of heavy breathing, pens scratching, that crooked analog clock ticking away whilst sitting loosely ontop of a pile of broken chairs and the loud yawns of senior citizens pacing around the room or completing their cross word puzzles.

No one spoke.

They weren't allowed to. Otherwise the seniors wouldn't get their cash for today. This was some ritual, where at the allocated time, the bells would ring and the silence would break. It was the indication that this ritual was complete. Life would resume very soon.

Bingo-like cards are then collected; carefully, slowly, meticulously. Order matters you see, their pay depended on it. The cryptic symbols alongside with the roman and greek alphabet meant that it was impossible for them to understand what theses cards were for; only that they would receive that cash for the days work.

> Comments: try _1st_ person present tense should have been used.
>
> -     what do people say? use speech

---

# Speech Feedback

You picked a relevant topic for your speech, finding a topic to talk about is tricky. Examples used were things we could relate to - coke versus Pepsi. You had more complex structure but it was easy to follow and I am amazed you remembered it all. A couple of things to think about are as follows:

- Put your paper down - you told us all that the speech was not on that piece of paper but you clung onto it during your speech - free up your hands if you are not using your paper - it also makes you come across as more relaxed. Notes are ok though as long as you don't read them word for word
- Watch out for other filler words apart from um and ahs. I noticed quick a few "actually" and "so" creeping in between ideas. It is really difficult but try to pause instead of using these Slow down a bit more in parts to vary the pace - it was quite quick
- You are a very confident speaker but any nerves you feel manifest themselves most in your breathing. Try to take a few deep breaths before you start and hopefully this will help

---

# Webpy Extending Wiki

Chapman's Web.py extended version is available [here](https://github.com/chappers/wiki).

[Skeleton](http://www.getskeleton.com/) was used to create responsive web format.

##Technical details

Since all updates are added in the database, to select the last updated the following SQL code was used:

    select
    	a.id,
    	a.url,
    	a.title,
    	a.content,
    	a.posted_on
    from
    	page as a,
    	(select
    		url,
    		max(posted_on) as posted_on
    		from page
    		group by url) b
    where
    	a.url = b.url
    	and a.posted_on = b.posted_on

---

# Failure Resume

Trying to debug jekyll? Try this: `jekyll --no-auto --server`

List of things to do to:

- Resume of my failures

---

# Webpy Extending To Do

The natural progression from Web.py [To Do list](http://webpy.org/src/todo-list/0.3) example, would be to extend it to include an edit function. Similar to the post above, I have used SQLite instead of Postgres. The repository is available [here](https://github.com/chappers/todo).

The edit functionality is included by adding an 'edit' url and the class named Edit.

urls:
urls = (
'/', 'Index',
'/del/(\d+)', 'Delete',
'/edit/(\d+)', 'Edit'
)

/templates/edit.html
$def with (todo, form)

    $todo.title

    <form action="" method="post">
    $:form.render()
    </form>

    class Edit
    class Edit:
    	form = web.form.Form(
    		web.form.Textbox('title', web.form.notnull,
    			description="I need to:"),
    		web.form.Button('Update'))

    	def GET(self, id):
    		todo = db.select('todo', where='id=$id', vars=locals())[0]
    		form = self.form
    		#put in the values of the form im editting
    		form.fill(todo)
    		return render.edit(todo, form)

    	def POST(self, id):
    		todo = db.select('todo', where='id=$id', vars=locals())[0]
    		form = self.form
    		if not form.validates():
    			return render.edit(todo, form)
    		#db.update('todo', where="id=$id",title = form.d.text, vars=locals())
    		db.update('todo', where="id=$id", vars=locals(),title=form.d.title)
    		raise web.seeother('/')

Simple!

---

# Sqlite and Webpy

`Web.py` tutorial and examples all come with the assumption that you would install `MySQL` or `Postgres`. Below I will demonstrate how you could complete the examples using `sqlite`, through python's default module `sqlite3`.

---

Database object is created using

    db = web.database(dbn='sqlite', db='dbname.db')

A table can be created using the SQL code below. Notice that there are sqlite specific code in the use of incrementation of 'id'.

    CREATE TABLE page (
    	page_id INTEGER PRIMARY KEY,
    	page_title TEXT,
    	page_content TEXT)

This idea can be applied to every example on the web.py website.
